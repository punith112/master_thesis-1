\chapter{Design\label{cha:chapter3}}



\section{Software Architecture of DAInamite}\label{sec:3.1}
DAInamite adopted modular programming pattern, so the tasks could be isolated in different modules. And more importantly, different people can focus on the development of its own module without interfering other modules. In addition to \cpp{}, Python is mainly used in the DAInamite team's code.
The time-critical components for motion, and vision are implemented in \cpp{}. The remaining modules such as localization, behavior, and ball-tracking are implemented in Python. The modules concerning the control of the robot will be connected to Naoqi. Naoqi is the software framework from Aldebaran, through which to directly control the NAO robot. A brief illustration of the physical architecture of the software is shown in \autoref{fig:architecture}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=.8\textwidth]{physical_architecture.png}
  \caption{Physical Architecture of DAInamite Code Base}
  \label{fig:architecture}
\end{figure}

\section{Vision Perception and Ground Truth}\label{sec:3.2}
\subsection{Vision Perception}\label{sub:vision}
As seen in \autoref{fig:architecture}, localization is one sub-module of pyagent module. In reality, localization is running as a separate thread at \unit[30]{Hz}. Localization module will be able to get odometry information from motion module and vision perception result from vision module. As the vision module process the raw image from the cameras, it extracts features such as field lines, field border, orange balls, yellow goals as shown in \autoref{fig:perception}. The localization module concerns only with the features from the vision result instead of the raw image.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.7\textwidth]{vision.png}
  \caption{The vision perception result of the robot in the field}
  \label{fig:perception}
\end{figure}

\subsection{Ground Truth}\label{sub:ground truth}
Before we dig into the implementation of localization, there is still one thing missing. Because in the end, the quality of the localization algorithm should be assessed. Thus, the true position of the robot needed to be obtained in order to do the comparisons and benchmarks. The true position of the robot or the so called \textit{ground truth} can not be obtained from the robot itself, since it does not have built in \gls{GPS} or other position tracking sensors. Moreover, the accuracy within centimeters is required for this purpose. 

The approach adopted in this paper is SSL-Vision \cite{zickler2010ssl}, the vision system used in RoboCup Small Size League to obtain the position of the robots. SSL-Vision requires a camera mounted on the ceiling, and a marker with specific pattern on top of the robot. By detecting the pattern through the ceiling camera, the ground truth can be obtained. For NAO robot, since its head could be scanning left and right, the marker can not be directly attached on its head, otherwise the robot's orientation obtained is not correct. To counter this, a plastic support is printed using 3D printer. As illustrated by \autoref{fig:collage}, the support is worn by the robot from the back, and the marker is attached on the top of the support.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.7\textwidth]{sslvision-collage.png}
  \caption{The 3D printed support and the pattern marker}
  \label{fig:collage}
\end{figure}

In the SSL-Vision software \cite{sslvision_yuan}, first set the field size, robot height, camera height and the corners of the field to calibrate the camera, so a point in the image plane can be mapped to the coordinate of the global frame. Then the colors in the field and the colors from the marker needed also to be calibrated. Shown in \autoref{fig:calibration} is the visualization result after calibration. When the marker is detected by SSL-Vision, the coordinate of the robot position in global frame will be broadcasted via network. The detected robot position is drawn in the field GUI in \autoref{fig:position}.\\

\begin{figure}[h!]
        \centering
        \begin{subfigure}[h]{0.59\textwidth}
                \includegraphics[width=\textwidth]{gt2.png}
                \caption{calibration result}
                \label{fig:calibration}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[h]{0.41\textwidth}
                \includegraphics[width=\textwidth]{gt1crop.png}
                \caption{position in field}
                \label{fig:position}
        \end{subfigure}%
	\caption{SSL-Vision color and camera calibration result (a), robot position result (b)}
	\label{fig:sslVision result}
\end{figure}


\noindent\textbf{Disadvantages of SSL-Vision}
\begin{itemize}
  \item The system highly depend on the light of the environment, once the surrounding light changes, the color metrics need to be recalibrated. 
  \item If the marker is printed using normal paper, it may cause reflection at certain angle from the view point of the camera, then the pattern can not be detected. For this reason, fuzzy materials are specially chosen to manually create the marker. 
  \item The system can not detect the pattern when the robot is fallen.
\end{itemize}

\subsection{Logging Perception and Ground Truth}
\label{sub:Logging Perception and Ground Truth}
For the purpose of debugging, the robot can stores the perception as log for future replay. The log contains the necessary data needed to re-run the localization algorithm on another computer. Within the log, it includes vision results, odometry, \gls{IMU}, sonar data, time stamp, robot posture, etc. In order to benchmark the quality of the localization algorithm result, the ground truth data needed to be stored as well, and when the log is replayed, the ground truth data should be synchronized with the perception log. The solution for this is to store the broadcasted ground truth from SSL-Vision at the same of recording the perception, and the ground truth become part of the perception log. The replayed perception log with ground truth is illustrated in \autoref{fig:logagent}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.7\textwidth]{logagent.png}
  \caption{perception log replayed with ground truth using particle filter localization (ground truth (red), calculated position (cyan), mirrored calculated position (pink))}
  \label{fig:logagent}
\end{figure}

\section{Robot Motion and Measurement}\label{sec:3.3}
As stated in \autoref{sub:kalman}, Kalman filter works by combining prediction step and measurement update to give an optimal estimate of the current state variable. In this paper, odometry information from motion module is used to perform the prediction step. From odometry, the information of how much distance the robot moved relative to the pose of last time stamp is provided, thus an estimate of the current position can be established. Likewise, an estimate from the measurement update is performed based on what the robot has observed, or simply put, the vision results from the cameras. In the end, Kalman filter update the state variable by a weighted average of the estimates.

The choice of motion model and observation model is fundamental for both Kalman filter's prediction step and measurement update. The more precise and comprehensive the model describes the system, the more accurate the estimate of the state will be. 

\subsection{Motion Model}\label{sub:Motion Model}
In general, motion models can be categorized by two kinds: velocity motion model and odometry motion model. Practical experience suggests that odometry, while still erroneous, is usually more accurate than velocity. Both suffer from drift and slippage, but velocity additionally suffers from the mismatch between the actual motion controllers and its mathematical model \cite{thrun2005probabilistic}. It is especially true for humanoid robot like NAO, whose moving velocity is difficult to model. 

Since the transformation between the coordinate used internally by the odometry measurement and the physical world coordinate is unknown, the internal odometry measurement in this motion model is relative. 
To be specific, in the time interval $(t-1, t]$, the robot moves from a position $x_{t-1}$ to position $x_t$, and meanwhile the odometry reports us a related movement from $\bar{x}_{t-1} = (\bar{x}, \bar{y}, \bar{\theta})$ to $\bar{x}_{t} = (\bar{x}', \bar{y}', \bar{\theta}')$. The bar here indicates that these are odometry measurements. We use the relative difference of $\bar{x}_{t-1}$ and $\bar{x}_{t}$ as an estimation of the difference between the true position $x_{t-1} = (x, y, \theta)$ and $x_t = (x', y', \theta')$. Therefore, the odometry information $u_t$ can be given by the pair:

\begin{equation}\label{eq:ut}
u_t = \begin{pmatrix}
\bar{x}_{t-1}\\ 
\bar{x}_{t}
\end{pmatrix}
\end{equation}

In our motion model, to obtain the relative odometry change, $u_t$ can be treated as a translation and then followed by a rotation. \autoref{fig:motion_model} demonstrates the decomposition of the odometry measurement. $\delta_{trans}$ is the translation and $\delta_{rot}$ is the rotation after translation. Both the translation and the rotation are considered under the coordinate $O$, which is relative to $\bar{x}_{t-1}$. The relative translation and rotation is calculated using \autoref{alg:substract poses}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.6\textwidth]{motion_modela.png}
  \caption{Relative odometry change. Translation $\delta_{trans}$, rotation $\delta_{rot}$ are considered in coordinate system $O$}
  \label{fig:motion_model}
\end{figure}

\begin{algorithm}                      
  \caption{substract\_poses ($\bar{x}_{t-1}, \bar{x}_{t}$)}         % give the algorithm a caption
\label{alg:substract poses}                           
\begin{algorithmic}[1]  
\State $\begin{bmatrix}
\delta_{x}\\ 
\delta_{y}
\end{bmatrix}
=
\begin{bmatrix}
\cos{\bar{\theta}} & \sin{\bar{\theta}}\\ 
-\sin{\bar{\theta}} & \cos{\bar{\theta}}
\end{bmatrix}\cdot 
\begin{bmatrix}
\bar{x}' - \bar{x}\\ 
\bar{y}' - \bar{y}
\end{bmatrix}$

\State $\delta_{rot} = \bar{\theta}' - \bar{\theta}$
\State $return (\delta_{x}, \delta_{y}, \delta_{rot})$

\end{algorithmic}
\end{algorithm}

Once the estimated translation $\delta_{trans}$ and rotation $\delta_{rot}$ relative to pose $x_{t-1}$ is obtained, it can be applied to update the robot position in the physical world coordinate. The full motion update using odometry measurement is shown in \autoref{alg:motion_update}.

\newcommand{\transfunc}{odometry\_motion\_update}

\begin{algorithm}                      
  \caption{\transfunc ($x_{t-1}$, $u_t$)}         % give the algorithm a caption
\label{alg:motion_update}                           
\begin{algorithmic}[1]                    
  %\State $\delta_{x}, \delta_{y}, \theta_{r} \gets \Call{sub_pose}{\bar{x}_{t-1}, \bar{x}_{t}}$
  \State $(\delta_{x}, \delta_{y}, \delta_{rot}) = substract\_poses (\bar{x}_{t-1}, \bar{x}_{t})$
  \State $
\begin{bmatrix}
  x'\\ 
  y'\\ 
\end{bmatrix}
= 
\begin{bmatrix}
  \cos\theta  & -\sin\theta \\ 
  \sin\theta &  \cos\theta  
\end{bmatrix}\cdot 
\begin{bmatrix}
  \delta_{x}\\ 
  \delta_{y} 
\end{bmatrix} +
\begin{bmatrix}
  x\\ 
  y 
\end{bmatrix}\
$
\State $\theta' = \theta + \theta_{rot}$ 
  %\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $return [x', y', \theta']^\top$
\end{algorithmic}
\end{algorithm}

The function \textit{\transfunc} in \autoref{alg:motion_update} corresponds to the transition function $f(x_{k-1}, u_{k})$ described in \gls{EKF} algorithm in \autoref{tab:ekf}. Then the related Jacobian Matrix $F_t$ can be calculated by \autoref{eq:jacobianF}.
\begin{equation}\label{eq:jacobianF} 
F_t = 
\begin{bmatrix}
1 & 0 & -sin(\theta)\cdot \delta_x - cos(\theta)\cdot\delta_y\\ 
0 &  1& cos(\theta)\cdot \delta_x  - sin(\theta)\cdot\delta_y\\ 
0 & 0 & 1
\end{bmatrix} 
\end{equation}

The process noise covariance matrix $Q_t$ is modeled to be positive proportional to the absolute change of odometry $(|\delta_{x}|, |\delta_{y}|, |\theta_{r}|)$. Since the more the robot moves in a time interval, the more it suffers from unpredictable noises like slippage and drift. \autoref{eq:Qt} is proposed to calculate process noise covariance matrix.
\begin{equation}\label{eq:Qt} 
Q_t = 
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix} 
\cdot
Sc^2 
\cdot
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix}^\top
\end{equation}

\begin{equation}\label{eq:Sc} 
Sc = \begin{bmatrix}
0.8 & 0.2 & 0.2\\ 
0.2 &  0.8 & 0.2\\ 
0.2 & 0.2 & 0.8
\end{bmatrix}
\end{equation}

$Sc$ is a scaling matrix and $Sc^2$ is square elementwise. It intuitively means, when robot walk \SI{1}{\meter} in x or y direction, it may have an error range in $\pm$ \SI{0.8}{\meter}. When it rotates \SI{1}{\radian}, it may have an error of $\pm$ \SI{0.8}{\radian} in orientation. The value of $Sc$ depends on the accuracy of odometry measurement from the motion module, and a better tuning of the values can be argued.

With all the essential elements for motion model ready, we plug in the formula from \autoref{tab:ekf} and perform the \gls{EKF} prediction step for the localization algorithm. The only thing we haven't touched yet is the initial value of $x_{t-1}$ and $P_{t-1}$, it will be discussed in ?(to be determined).  



\subsection{Observation Model}\label{sub:Sensor Model}



