\chapter{Background and Related Work\label{cha:chapter2}}

%Many researches have been done regarding robot self-localization in various application domains. 
%Most of the localization algorithm fall into the class of paricle filter or Kalman filter variants like \gls{EKF} and \gls{UKF}.
In robotics, significant researches have been carried out to make the robot localize itself in the environment more accurately. Many localization filters based on probability theories are proposed. Most of them draw their fundamental theory from Bayers filters, and later they evolve into more specific filters such as Particle filters \cite{dellaert1999monte} or Kalman filters \cite{kalman1960new} variants like \gls{EKF} \cite{julier1997new} and \gls{UKF} \cite{van2001square}. Each kind of filter has its own intrinsic properties and has pros and cons depend on the actual application.

\section{Bayes Filters}\label{sec:2.1}
Bayes filter aims to sequentially estimate the belief of the state conditioned on all the information obtained in the sensor data. As illustration, assume the sensor data has a sequences of time-indexed sensor data. The belief of state $x(t)$ $Bel(x_{t})$ can be represented as a posterior probability density conditioned on all the sensor data from the past observations, $z_{1},z_{2} \ldots z_{t}$.
$$Bel(x_{t}) = p(x_{t}|z_{1}, z_{2}, \ldots, z_{t})$$
However, the computation complexity of such belief is growing exponentially with the increase of observations. To make the computation feasible, Bayes filters assume that the dynamic system is Markov. It means the state belief at time $t$ only depends only on the previous state information at time $t-1$. Bayes filter includes two components to estimate the state belief.
\\
\\
\noindent \textit{Time update (prediction)}\\
Before the sensor measurement, a prediction is made by using the motion model of the robot
$$Bel^{-}(x_{t}) \leftarrow \int p(x_{t}|x_{t-1})Bel(x_{t-1})dx_{t-1}.$$
Here $p(x_{t}|x_{t-1})$ depicts the system's dynamics in which a motion model is used to describe how the system state change due to motion movement.
\\
\\
\noindent \textit{Measurement update (correction)}\\
By making a measurement from the sensor, the filter correct the prediction using this observation.
$$Bel(x_{t}) \leftarrow \alpha_{t}p(z_{t}|x_{t})Bel^{-}(x_{t})$$
$p(z_{t}|x_{t})$ represent the probability of making observation $z_{t}$ given that the robot is at position $x_{t}$. It generally describes the perceptual model of the robot. Coefficient $\alpha_{t}$ is a normalization factor to make the Belief calculated add up to 1.

Moreover, the update of the belief in Bayes filter is recursive, which means the belief at time $t$ is calculated from belief at time $t-1$. This process continues recursively. Bayes Filter provides the theoretical basis for the following filters to be discussed, namely particle filters and Kalman filters. 

\subsection{Particle Filter}
Particle Filter, also known as \gls{MCL}, is the localization algorithm that is currently under use for the \gls{SPL} robot in DAInamite. Particle filter represent a probability distribution by a set of samples or particles. Each particle represents a concrete state in the current system at time $t$, in our case it will be the one instantiation of coordinates and the orientation. Meanwhile each particle is also associated with a weight which indicates how probable the state is in the system. In practice, the number of particles is huge in order to provide a close hypothesis for the true world, and normally 1000 particles are needed to approximate the \gls{SPL} soccer field. Following the principle of Bayes filters, the motion update will move the particles according to the motion model; measurement update will result in a resampling of the particles, in which the particles with higher weigh will survive, on the other hand, the ones with lower weight will die out, and in replacement particles close to the area of the surviving ones will be generated. In the DAInamite \gls{SPL} robot, due to limited computation power of NAO, only 60 particles are instantiated in the system, with such low number of particles, the system state can hardly being estimated. As an enhancement for this, the technique of Particle filter resetting is used to improve the performance of the localization algorithm with low number of particles. It produces particles with high weights for locations where unambiguous observations are made. The high weight particles help to provide relatively reliable base states which help to result in correct location interpretation in the later recursive processing. 
Advantages

\noindent\textbf{Advantages}
\begin{itemize}
  \item  Particle filter is easy to program
  \item Particle filter is robust in localization 
  \item  Particle filter works with multi-modal probability distribution and is applicable in both linear and non-linear system
\end{itemize}

\noindent\textbf{Disadvantages}

\begin{itemize}
  \item  Computationally intensive when particle number is high.
  \item  Estimation accuracy reduces when number of particles is low.
\end{itemize}


\subsection{Kalman Filter}
Kalman filter was invented by Swerling (1958) and Kalman (1960) as a technique for filtering and prediction in linear Gaussian systems. In Kalman filter, it assumes the belief of a state conforms to a Gaussian distribution.

A Gaussian distribution for random a single variable x has the form
$$f(x,\mu,\sigma) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}$$
The parameter $\mu$ in this definition is the mean or expectation of the distribution. The parameter $\sigma$ is its standard deviation; its variance is therefore $\sigma ^2$ .The larger the variance is, the wider the distribution will spread. However, in the problem domain of the robot localization, the state of the robot is a vector with three variables 2D coordinate and orientation. To represent a multivariate Gaussian distribution, the following definition is used:
$$f_{x}(x_1,\ldots, x_k) = \frac{1}{\sigma \sqrt {(2\pi)^k|P|}}exp(-\frac{1}{2}(x-\mu)^TP^{-1}(x-\mu))$$

It mimics the form of its one dimensional counterpart. $x$ represent the state vector, $\mu$ is the expectation of the distribution, $P$ represents a covariance matrix which is positive-semidefinite and symmetric. If the state is of three variables, then $P$ is a $3\times3$ matrix.

The Kalman filter model assumes the true state at time k is evolved from the state at $(k-1)$ according to
$$x_k = F_kx_{k-1}+B_k\mu_k+w_k$$

where
\begin{itemize}
  \item $F_k$ is the state transition model which is applied to the previous state $x_{k-1}$;  
  \item $B_k$ is the control-input model which is applied to the control vector $u_k$;
  \item $w_k$ is the process noise which is assumed to be drawn from a zero mean multivariate normal distribution with covariance $Q_k$.
\end{itemize}
$$w_k \sim N(0,Q_k)$$
At time $k$ an observation (or measurement) $z_k$ of the true state $x_k$ is made according to
$$z_k = H_kx_k + v_k$$
Where $H_k$ is the observation model which maps the true state space into the observed space and $v_k$ is the observation noise which is assumed to be zero mean Gaussian white noise with covariance $R_k$.
$$v_k \sim N(0,R_k)$$
Since Kalman filter is a variant implementing the Bayes filters, it follows the prediction-correction routine defined in Bayes filters.
\\
\\
\noindent\textit{Prediction}\\
Predicted (a priori) state estimate $\hat{x}_{k\mid k-1} = F_{k}\hat{x}_{k-1\mid k-1} + B_{k} u_{k}$ \\
Predicted (a priori) estimate covariance $P_{k\mid k-1} =  F_{k} P_{k-1\mid k-1} F_{k}^{\text{T}} + Q_{k}$\\
\\
\noindent\textit{Measurement Update}\\
Innovation or measurement residual $\tilde{y}_k = z_k - H_k\hat{x}_{k\mid k-1}$ \\
Innovation (or residual) covariance $S_k = H_k P_{k\mid k-1} H_k^\text{T} + R_k$ \\
Optimal Kalman gain $K_k = P_{k\mid k-1}H_k^\text{T}S_k^{-1}$ \\
Updated (a posteriori) state estimate $\hat{x}_{k\mid k} = \hat{x}_{k\mid k-1} + K_k\tilde{y}_k$ \\
Updated (a posteriori) estimate covariance $P_{k|k} = (I - K_k H_k) P_{k|k-1}$ \\

\noindent\textbf{Advantages}
\begin{itemize}
  \item  It is known from the theory that the Kalman filter is an optimal estimator in case that a) the model perfectly matches the real system, b) the entering noise is white and c) the covariances of the noise are exactly known.
  \item  It is computational efficient for the prediction and correction step, because the calculation can be done efficiently using matrix multiplication
\end{itemize}

\noindent\textbf{Disadvantages}
\begin{itemize}
  \item  Kalman filter works well only with linear system dynamics. In the SPL, neither the motion model nor the observation model of the robot is linear, thus the state transition equation defined above cannot be straightforwardly used. Others techniques to deal with non-linearity proposed in Extended Kalman filter and Unscent Kalman filter needed to be applied.
  \item  Kalman filter works with uni-modal probability distribution, in other words, uni-modal means there is only a single highest value in the probability distribution under consideration. However, due to the ambiguity of the vision data input, there could be multiple locations which are the potential candidates. In this case, the probability distribution is multi-modal.
\end{itemize}


\subsection{Multi-Model Kalman Filter}
While in the realm of robot system, the sensor model and motion model for robot localization are hardly linear, many other theories like \gls{EKF} has been proposed which approximate the system to be linear so that the equations from Kalman filer can be utilized. However since \gls{EKF} assume the posterior probability is still Gaussian distribution, it performs rather like a maximum likelihood estimator than as a minimum variance estimator~\cite{alspach1972nonlinear}. As a result, it greatly reduces the amount of information which is in the true density distribution which might be multi-modal. To overcome the shortcoming of Kalman filter and its ramifications which perform poorly in multi-modal probability distribution, Multi-Model Kalman Filter was proposed in~\cite{alspach1972nonlinear}. Multi-Model Kalman Filter represent the probability distribution using the weighted sum of multiple Gaussian distribution models, and each Gaussian distribution model $i$ has its weight $\alpha_i$, for $\alpha_i \in $[0, 1].

For each model, the multivariate normal probability distribution function(pdf) is given by:

$$p_{i}(x) = \alpha_i\frac{1}{\sigma \sqrt {(2\pi)^k|P_i|}}exp(-\frac{1}{2}(x-\mu)^TP_i^{-1}(x-\mu))$$

The overall mixture pdf is therefore:
$$p(x) = \sum_{i=1}^{N}p_i(x)$$
The mixture of Gaussian representations allows to construct a sensor model with one Gaussian term for each possible correspondence.

During the measurement update, the weight $\alpha_i$ is updated using the method according to ~\cite{alspach1972nonlinear}:

$$\alpha_{i}(x) = \nu\alpha_i'\frac{1}{\sigma \sqrt {(2\pi)^k|P_\eta|}}exp(-\frac{1}{2}\eta^{-1}P_\eta^{-1}\eta)$$

Where $\eta = (z_t - z\widehat{}_i)$indicate the $m$-dimensional innovation vector between the measured observation $z_t$ and the expected observation $z\widehat{}_i$ for the fixed correspondence according the $i$'th model. $P_\eta$ is the sum of the measurement and the prediction covariance, and $\nu$ is a normalization factor.

There exist two situations to be considered for measurement update for multi-model Kalman filter. One is when the observation is unambiguous, which means when goal post or unique land marks have been detected, so only one approximation of the robot state will be generated from the vision. In this case, the measurement update will be applied on each Gaussian term and result in the same number of terms as before. Another case is when the vision measurement is ambiguous, for example, it may generate N approximations of the robot state from the vision input, because the landmark is not unique or cannot provide enough information. In this case, suppose initially the system has M Gaussian models, after update the system will split into M*N Gaussian models. With ambiguous measurement update the terms of Gaussian are increasing multiplicatively. Methods needed to be applied to prune the growing number of Gaussian models. Model merge equations and decisions has been proposed in \cite{Quinlan2010}, in which several Gaussian models are merged into one if some metrics are met. In \cite{Jochmann2012}, for efficiency, the author only takes the Gaussian models with maximum likelihood, and compensates the information which is lost when models are pruned by introducing filter resetting technique to reproduce the missing models, which is an ingredient taken from particle filter theory.

\section{Prior work of Localization for robot in RobotCup}\label{sec:2.2}
Most localization algorithms applied in RoboCup \gls{SPL} falls into the class of either paricle filter or Kalman filter based localization. Quinlan and Middleton from Team RoboEireann use multiple model Kalman filters for robot localization \cite{Quinlan2010}. They split the gaussian model when ambiguous measurement happens to cover the multi-modal hypothesis distribution. The problem this strategy brings is the multiplicatively increasing number of Kalman models during model split, therefore a model merge step is proposed to combine the models. Team NaoDevil also baed their localization on a multi-hypothesis \gls{UKF}, in stead of model splitting they adopt sensor resetting technique which commonly used in particle filter \cite{Jochmann2012}. 

However paricle filter or Kalman filter is not the only solution for localization, team Berlin United also proposed a constraint based world modeling for localization \cite{Gohring2009} to overcome the reduction of landmarks such as beacons or colored goals in RoboCup. They build robot position constrains by using the geometry property of the lines in the field. In midsize league, a localizaiton method based on optimization approach \cite{Lauer2006} is proposed, it tries to calculate the perfect match of the vision with the field. Then the pose of the robot is also determined.
