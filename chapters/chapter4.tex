\chapter{Localization Design\label{cha:chapter4}}


As stated in \autoref{sub:kalman}, Kalman filter works by combining prediction step and measurement update to give an optimal estimate of the current state variable. In this paper, odometry information from motion module is used to perform the prediction step. From odometry, the information of how much distance the robot moved relative to the pose of last time stamp is provided, thus an estimate of the current position can be established. Likewise, an estimate from the measurement update is performed based on what the robot has observed, or simply put, the vision results from the cameras. In the end, Kalman filter update the state variable by a weighted average of the estimates.

The choice of motion model and observation model is fundamental for both Kalman filter's prediction step and measurement update. The more precise and comprehensive the model describes the system, the more accurate the estimate of the state will be. 

\section{Motion Update}\label{sec:Motion Update}
\subsection{Motion Model}\label{sub:Motion Model}
In general, motion models can be categorized by two kinds: velocity motion model and odometry motion model. Practical experience suggests that odometry, while still erroneous, is usually more accurate than velocity. Both suffer from drift and slippage, but velocity additionally suffers from the mismatch between the actual motion controllers and its mathematical model \cite{thrun2005probabilistic}. It is especially true for humanoid robot like NAO, whose moving velocity is difficult to model. 

Since the transformation between the coordinate used internally by the odometry measurement and the physical world coordinate is unknown, the internal odometry measurement in this motion model is relative. 
To be specific, in the time interval $(t-1, t]$, the robot moves from a position $x_{t-1}$ to position $x_t$, and meanwhile the odometry reports us a related movement from $\bar{x}_{t-1} = (\bar{x}, \bar{y}, \bar{\theta})$ to $\bar{x}_{t} = (\bar{x}', \bar{y}', \bar{\theta}')$. The bar here indicates that these are odometry measurements. We use the relative difference of $\bar{x}_{t-1}$ and $\bar{x}_{t}$ as an estimation of the difference between the true position $x_{t-1} = (x, y, \theta)$ and $x_t = (x', y', \theta')$. Therefore, the odometry information $u_t$ can be given by the pair:

\begin{equation}\label{eq:ut}
u_t = \begin{pmatrix}
\bar{x}_{t-1}\\ 
\bar{x}_{t}
\end{pmatrix}
\end{equation}

In the following, let \autoref{eq:rot} denote the rotation matrix with angle $\alpha$.
\begin{equation}\label{eq:rot}
\Omega(\alpha) = \begin{bmatrix}
\cos{\alpha} &  -\sin{\alpha} \\
\sin{\alpha} &  \cos{\alpha}
\end{bmatrix}
\end{equation}

In our motion model, to obtain the relative odometry change, $u_t$ can be treated as a translation and then followed by a rotation. \autoref{fig:motion_model} demonstrates the decomposition of the odometry measurement. $\delta_{trans}$ is the translation and $\delta_{rot}$ is the rotation after translation. Both the translation and the rotation are considered under the coordinate $O$, which is relative to $\bar{x}_{t-1}$. The relative translation and rotation is calculated using \autoref{alg:substract poses}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.6\textwidth]{motion_modela.png}
  \caption{Relative odometry change. Translation $\delta_{trans}$, rotation $\delta_{rot}$ are considered in coordinate system $O$}
  \label{fig:motion_model}
\end{figure}

\begin{algorithm}                      
  \caption{subtract\_poses ($\bar{x}_{t-1}, \bar{x}_{t}$)}         % give the algorithm a caption
\label{alg:substract poses}                           
\begin{algorithmic}[1]  
\State $\begin{bmatrix}
\delta_{x}\\ 
\delta_{y}
\end{bmatrix}
=
\Omega(-\bar{\theta})
%\begin{bmatrix}
%\cos{\bar{\theta}} & \sin{\bar{\theta}}\\ 
%-\sin{\bar{\theta}} & \cos{\bar{\theta}}
%\end{bmatrix}\cdot 
\begin{bmatrix}
\bar{x}' - \bar{x}\\ 
\bar{y}' - \bar{y}
\end{bmatrix}$

\State $\delta_{rot} = \bar{\theta}' - \bar{\theta}$
\State $return (\delta_{x}, \delta_{y}, \delta_{rot})$

\end{algorithmic}
\end{algorithm}

Once the estimated translation $\delta_{trans}$ and rotation $\delta_{rot}$ relative to pose $x_{t-1}$ is obtained, it can be applied to update the robot position in the physical world coordinate. The full motion update using odometry measurement is shown in \autoref{alg:motion_update}.

\newcommand{\transfunc}{odometry\_motion\_update}

\begin{algorithm}                      
  \caption{\transfunc ($x_{t-1}$, $u_t$)}         % give the algorithm a caption
\label{alg:motion_update}                           
\begin{algorithmic}[1]                    
  %\State $\delta_{x}, \delta_{y}, \theta_{r} \gets \Call{sub_pose}{\bar{x}_{t-1}, \bar{x}_{t}}$
  \State $(\delta_{x}, \delta_{y}, \delta_{rot}) = substract\_poses (\bar{x}_{t-1}, \bar{x}_{t})$
  \State $
\begin{bmatrix}
  x'\\ 
  y'\\ 
\end{bmatrix}
= 
\Omega(\theta)
% \begin{bmatrix}
%   \cos\theta  & -\sin\theta \\ 
%   \sin\theta &  \cos\theta  
% \end{bmatrix}
\cdot 
\begin{bmatrix}
  \delta_{x}\\ 
  \delta_{y} 
\end{bmatrix} +
\begin{bmatrix}
  x\\ 
  y 
\end{bmatrix}\
$
\State $\theta' = \theta + \theta_{rot}$ 
  %\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $return [x', y', \theta']^\top$
\end{algorithmic}
\end{algorithm}

The function \textit{\transfunc} in \autoref{alg:motion_update} corresponds to the transition function $f(x_{k-1}, u_{k})$ described in \gls{EKF} algorithm in \autoref{tab:ekf}. Then the related Jacobian Matrix $F_t$ can be calculated by \autoref{eq:jacobianF}.
\begin{equation}\label{eq:jacobianF} 
F_t = 
\begin{bmatrix}
1 & 0 & -sin(\theta)\cdot \delta_x - cos(\theta)\cdot\delta_y\\ 
0 &  1& cos(\theta)\cdot \delta_x  - sin(\theta)\cdot\delta_y\\ 
0 & 0 & 1
\end{bmatrix} 
\end{equation}

\subsection{Process Noise Model}
\label{sub:Process Noise Model}

The process noise covariance matrix $Q_t$ is modeled to be positive proportional to the absolute change of odometry $(|\delta_{x}|, |\delta_{y}|, |\theta_{r}|)$. Since the more the robot moves in a time interval, the more it suffers from unpredictable noises like slippage and drift. \autoref{eq:Qt} is proposed to calculate process noise covariance matrix.
\begin{equation}\label{eq:Qt} 
Q_t = 
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix} 
\cdot
Sc^2 
\cdot
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix}^\top
\end{equation}

\begin{equation}\label{eq:Sc} 
Sc = \begin{bmatrix}
0.8 & 0.2 & 0.2\\ 
0.2 &  0.8 & 0.2\\ 
0.2 & 0.2 & 0.8
\end{bmatrix}
\end{equation}

$Sc$ is a scaling matrix and $Sc^2$ is square elementwise. It intuitively means, when robot walk \SI{1}{\meter} in x or y direction, it may have an error range in $\pm$ \SI{0.8}{\meter}. When it rotates \SI{1}{\radian}, it may have an error of $\pm$ \SI{0.8}{\radian} in orientation. The value of $Sc$ depends on the accuracy of odometry measurement from the motion module, and a better tuning of the values can be argued.

With all the essential elements for motion model ready, we plug in the formula from \autoref{tab:ekf} and perform the \gls{EKF} prediction step for the localization algorithm. The only thing we haven't touched yet is the initial value of $x_{t-1}$ and $P_{t-1}$, it will be discussed in ?(to be determined).  



\section{Sensor Update}\label{sec:sensor update}
The robot can also measure its position by sensing the surrounding environment. In the case of NAO robot, the measurement is made by observing the environment from its two cameras. The images from the cameras are preprocessed by the DAInamite's vision module to extract the landmarks as shown in \autoref{fig:perception}. During the work of this thesis, two sensor update models are proposed to accomplish the localization task. One is based on an optimization approach and another is based on features. We will discuss both models in this section.

\subsection{Optimization Based Model}\label{sec:optimization update} 
In the early phase of the thesis, the only unique landmark can be detected by vision module is the yellow goal posts, however, from 2015 the yellow goal post will be replaced by white goal post according to the rule \cite{Committee2013}. Then the only landmark available is the white line in the field. Field lines are of high ambiguity, in the case of feature based Kalman filter localization, a wrong matching of the landmarks could lead to the divergence of the tracking which can hardly be recovered. Thus, a robust and accurate algorithm for sensor update is implemented taking the reference from Lauer \cite{Lauer2006}. The algorithm is based on Rprop \cite{Riedmiller1993} optimization method, which uses only the line points from the detected lines, to find the robot position which matches the field map best. 



% \subsubsection{Image Frame to Robot Frame}
% 
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=.8\textwidth]{camera-matrix.png}
%   \caption{coordinate of the bottom camera and the robot local coordinate}
%   \label{fig:motion_model}
% \end{figure}


\subsection{Feature Based Model}\label{sec:feature update} 
Although the optimization method based localization outperforms the particle filter in terms of accuracy, it is not computational efficient to run on the robot. During the preparation of German Open 2015, DAInamite team developed more feature detections like center circle detection, penalty area detection and ``L'' junction detection. Provided the richness of features, the development of feature based sensor update for localization is motivated.  

\subsubsection{``T'' and ``X'' Junction Detection}
In the \gls{SPL} soccer field, there are 36 ``L'' junctions, 14 ``T'' junctions, 2 ``X'' junctions \footnote{The detection of ``L'' consider both side of the field line as well as the line direction, ``T''and ``X'' junctions do not consider line side and direction. The counting includes ``L'', ``T'', ``X'' junctions at center circle}. Compared to ``L'' junction, ``T'' and ``X'' junctions are much stronger landmarks. Especially the ``T'' junction at the penalty area, it can significantly help the robot localize itself in front of the goal.
Both the ``T'' and ``X'' junctions are detected in the robot frame instead of the image plane, since in the image plane the perpendicularity of the lines can not be easily observed. When the detected lines are projected to the robot frame, every two lines are compared according to their angles. If the differences of the angles are close to \SI{90}{\degree}, then the intersection point of the two lines are computed assuming the lines have infinite length. After the intersection is calculated, it should be satisfied that the intersection lies on at least one of the line segments. The distance from line end points to the intersections are further checked if they satisfy the threshold according to T or X geometry. The pseudocode of the TX junction detection is illustrated in \autoref{alg:txdetection}, and the detection result is shown in \autoref{fig:txcorner}.

\begin{algorithm}                      
  \caption{TX\_junction\_detection ()}         % give the algorithm a caption
\label{alg:txdetection}                           
\begin{algorithmic}[1]                    
  \For{each $Line(i)$}
  	\For{each $Line(j)$}
		\If {\Call{AngleDifference}{$Line(i)$, $Line(j)$} $\approx$ \SI{90}{\degree}}
		\State $intersection \gets$ \Call{Intersection}{$Line(i)$, $Line(j)$}
		    \If {$intersection$ lies on both $LineSegment(i)$ and $LineSegment(j)$}
			\If {\Call {CheckDistance}{$LineEndPoints(i)$, $LineEndPoints(j)$, $intersection$} within ThresholdX}
				\State $LineSegment(i)$ and $LineSegment(j)$ is a X junction
			\EndIf
		    \ElsIf{$intersection$ lies on either $LineSegment(i)$ or  $LineSegment(j)$}
			\If {\Call {CheckDistance}{$LineEndPoints(i)$, $LineEndPoints(j)$, $intersection$} within ThresholdT}
				\State $LineSegment(i)$ and $LineSegment(j)$ is a T junction
			\EndIf
		    \EndIf
		\EndIf
	\EndFor
  \EndFor
\end{algorithmic}
\end{algorithm}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\textwidth]{TXLcorner_crop.png}
  \caption{Detection of ``L''(Cyan), ``T''(Red) and ``X''(Yellow) junction in the field. The purple ellipse indicates the measurement covariance}
  \label{fig:txcorner}
\end{figure}

\subsubsection{Measurement Model Choice}\label{subsub:sensorModelChoice}
Given the various kinds of features, different sensor model can be distinguished for different features. In general, they can be divided into point Landmarks and line Landmarks. Assume the robot position $x_t = (p_x, p_y, p_{\theta})$, and the landmark represented as $l$. The objective is to describe the expected measurement $\bar{z}$ by the observation function $h(x_t)$ and derive the Jacobian matrix $H$.
\paragraph{Point Landmark without orientation}\label{par:pointLandmark}
Under the Cartesian coordinate system in robot frame. The expected point landmark can be described as $\bar{z} = (z_x, z_y)^\top$ (illustrated in \autoref{fig:observation_euclidean}) and its corresponding landmark in physical world frame $l = (l_x, l_y)^\top$. The observation function $h(x_t)$ can be described in \autoref{eq:hx}, which transforms the landmark from physical world frame to the robot local frame. The Jacobian matrix $H$ is the partial derivative of $h(x_t)$ at $x_t$, as illustrated in \autoref{eq:jH}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=.8\textwidth]{observation_euclidean.png}
  \caption{Observation given in Cartesian coordinates \cite{Tasse2013}}
  \label{fig:observation_euclidean}
\end{figure}

\begin{equation}\label{eq:hx}
  \bar{z} = \begin{pmatrix}
\bar{z}_{x}\\ 
\bar{z}_{y}
\end{pmatrix}
= 
h(x, l)
=
\Omega(-p_{\theta})
\cdot
\begin{bmatrix}
 \begin{pmatrix}
l_{x}\\ 
l_{y}
\end{pmatrix}
-
\begin{pmatrix}
p_{x}\\ 
p_{y}
\end{pmatrix}
\end{bmatrix}
\end{equation}

\begin{equation}\label{eq:jH}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  -\cos{p_{\theta}} & -\sin{p_{\theta}} & -(l_x-p_x)\sin{p_{\theta}} + (l_y-p_y)\cos{p_{\theta}} \\ 
  \sin{p_{\theta}} & -\cos{p_{\theta}} & -(l_x-p_x)\cos{p_{\theta}} - (l_y-p_y)\sin{p_{\theta}} 
\end{bmatrix}
\end{equation}
The feature which follows this model is center circle without the center line being detected.

\paragraph{Point Landmark with Orientation}\label{par:pointLandmark}
When the point landmark has not only position but also orientation, it can be described as $l = (l_x, l_y, l_{\theta})^\top$ in the physical world frame, and the expected point landmark can be described as $\bar{z} = (z_x, z_y, z_{\theta})^\top$.  With addition of orientation, the observation function $h(x_t)$ can be described in \autoref{eq:hx2} and the Jacobian matrix $H$ in \autoref{eq:jH2}.

\begin{equation}\label{eq:hx2}
  \bar{z} = \begin{pmatrix}
\bar{z}_{x}\\ 
\bar{z}_{y} \\
\bar{z}_{\theta} 
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}

\Omega(-p_{\theta})
\cdot
\begin{bmatrix}
 \begin{pmatrix}
l_{x}\\ 
l_{y}
\end{pmatrix}
-
\begin{pmatrix}
p_{x}\\ 
p_{y}
\end{pmatrix}
\end{bmatrix} \\
l_{\theta} - p_{\theta}
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jH2}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  -\cos{p_{\theta}} & -\sin{p_{\theta}} & -(l_x-p_x)\sin{p_{\theta}} + (l_y-p_y)\cos{p_{\theta}} \\ 
  \sin{p_{\theta}} & -\cos{p_{\theta}} & -(l_x-p_x)\cos{p_{\theta}} - (l_y-p_y)\sin{p_{\theta}}  \\
  0 & 0 & -1
\end{bmatrix}
\end{equation}
The landmarks which comply with is measurement model are ``L'',``T'',``X'' junctions, and center circle with center line detected.

\paragraph{Line Landmark}\label{par:lineLandmark}
Line is a special landmark, it needs two end points to describe it in Cartesian space. However, with only two end points of the line, it is very hard to find the corresponding points in the field map. For example, a short line can be matched to any segment within a long line. Therefore, to counter this ambiguity and extract the intrinsic property from lines, the representation from Hough transform is used where a line can be represented as ($\rho$, $\theta$), as shown in \autoref{fig:hough}. 
\begin{figure}[h!]
  \centering
 \begin{tikzpicture}[
     scale=5,
     axis/.style={very thick, ->, >=stealth'},
     important line/.style={thick},
     dashed line/.style={dashed, thin},
     pile/.style={thick, ->, >=stealth', shorten <=2pt, shorten
     >=2pt},
     every node/.style={color=black}
     ]
     % axis
     \draw[axis] (0,0) coordinate (O) -- (1.1,0) coordinate(x) node(xline)[right]
         {$x$};
     \draw[axis] (0,0) -- (0,1.1) coordinate(y) node(yline)[left] {$y$};
     % Lines
 
     \draw[important line] (0.1,0.942) coordinate (A) -- (0.832,0.519) coordinate (B) node[right, text width=5em] {\textit{Line}};
 
     \draw[dashed] (O) -- (0.433, 0.75) coordinate(C);
     \draw (0.216, 0.5) node() {$\rho$};	
     \draw pic[ draw=orange, <->, angle eccentricity=1.2, angle radius=1cm]
     {angle=x--O--C};

     \draw (0.2,0.1) node()[right] {$\theta$};
     \def\ralen{.5ex}  % length of the short segment
     \foreach \inter/\first/\last in {C/O/B}
     {
       \draw let \p1 = ($(\inter)!\ralen!(\first)$), % point along first path
       \p2 = ($(\inter)!\ralen!(\last)$),  % point along second path
       \p3 = ($(\p1)+(\p2)-(\inter)$)      % corner point
       in
       (\p1) -- (\p3) -- (\p2) ;              % path
       %($(\inter)!.5!(\p3)$) node [dot] {};  % center dot
     }
   \end{tikzpicture}
   \caption{Line represented in Hough space}
   \label{fig:hough}
 \end{figure}
While Hough representation discards the length information of the line, it keeps the direction and distance information from the line intact. It means, for example, when a vertical line is matched, the robot's orientation can be determined and it will also be sure about its position in $x$ axis, but keep uncertain about its position in $y$ axis until a horizontal line is matched.

Using Hough representation, the expected line landmark can be described as $\bar{z} = (z_{\rho}, z_{\theta})^\top$, and depending on whether the corresponding line in the field map is vertical or horizontal, different measurement model should be adjusted. If knowing the matched line is horizontal, the line landmark in physical world frame can be describe using only $y$ axis value $l = l_{y}$. Then the observation function $h(x_t)$ can be described in \autoref{eq:hxhorizon} and the Jacobian matrix $H$ in \autoref{eq:jHhorizon}. \\
\begin{equation}\label{eq:hxhorizon}
  \bar{z} = \begin{pmatrix}
\bar{z}_{\rho}\\ 
\bar{z}_{\theta}
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}
|l_{y}-p_{y}| \\
atan2(\cos{p_{\theta}\cdot (l_y-p_y)}, \sin{p_{\theta}}\cdot (l_y-p_y))
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jHhorizon}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  0 & \frac{l_{y}-p_{y}}{|l_{y}-p_{y}|} & 0 \\ 
  0 & 0 & -1 
\end{bmatrix}
\end{equation}

Similar for vertical lines, the line landmark in physical world frame can be describe using only $x$ axis value $l = l_{x}$. The observation function $h(x_t)$ can be described in \autoref{eq:hxvertical} and the Jacobian matrix $H$ in \autoref{eq:jHvertical}. \\
\begin{equation}\label{eq:hxvertical}
  \bar{z} = \begin{pmatrix}
\bar{z}_{\rho}\\ 
\bar{z}_{\theta}
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}
|l_{x}-p_{x}| \\
atan2(\cos{p_{\theta}\cdot (l_x-p_x)}, -\sin{p_{\theta}}\cdot (l_x-p_x))
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jHvertical}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  \frac{l_{x}-p_{x}}{|l_{x}-p_{x}|} & 0 & 0\\ 
  0 & 0 & -1 
\end{bmatrix}
\end{equation}

The features fall into this measurement model is the detected lines as well as the penalty area. Because the penalty area detection basically is just the detection of two parallel lines which satisfies certain criteria. The penalty area detection result is given in the form of Hough representation, thus it is also treated as a line.

\subsubsection{Measurement Noise Model}
\label{sub:Measurement Noise Model}
The measurement noise indicates how accurate the measurement made for each feature is, and by assuming the noise distribution is Gaussian, the measurement noise covariance is represent by the term $R_{t}$ in \autoref{tab:ekf}. The Measurement noise in case of NAO robot in \gls{SPL} game can mainly come from:
\begin{itemize}
  \item inaccuracy of the camera matrix
  \item blur of the image due to motion
\end{itemize}
Since all the landmarks are measured in robot frame, the further the landmarks from the robot (the origin of robot frame coordinate), the larger the measurement noise will be. Because the error introduced by the camera matrix will be magnified when the detected landmarks are projected from image plane to robot frame. This phenomenon is demonstrated in \autoref{fig:landmarkImagePlane} and \autoref{fig:landmarkRobotFrame}. For example, the line (both sides) which being detected from the penalty area in \autoref{fig:landmarkImagePlane} become two lines with big gap (shown in \autoref{fig:landmarkRobotFrame}) when projected to the robot frame. Both lines suffered significant error in distance and angle.\\

\begin{figure}[h!]
  \centering
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.8\textwidth]{measure_camera.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[red,ultra thick, dashed, rounded corners] (0.179,0.59) rectangle (0.459,0.53);
    \end{scope}
\end{tikzpicture}
  \caption{Landmarks in image plane. (The red box indicates the lines which detected from the front line of the penalty area)}
  \label{fig:landmarkImagePlane}
\end{figure}


\begin{figure}[h!]
  \centering
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{measure_err.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[red,ultra thick, dashed, rounded corners] (0.841,0.62) rectangle (0.918,0.284);
    \end{scope}
\end{tikzpicture}
  \caption{Landmarks in robot frame. (Grey lines indicate the detected lines after projection, the red box indicates the lines which detected from the front line of the penalty area)}
  \label{fig:landmarkRobotFrame}
\end{figure}

A straight forward way to describe this phenomenon is to model the noise proportional to the distance of the landmark in robot frame. However, the problem with this model is that it is hard to choose a physically meaningful coefficient for the linear function. Therefore, another similar but more suitable noise model which is used by team B-Human is adopted \cite{Bhuman}. (figure needed to describe)

\begin{figure}[h!]
\begin{center}
  \begin{tikzpicture}[
     axis/.style={very thick, ->, >=stealth'},
    ]
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.75\textwidth]{simspark.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
      %\draw[red,ultra thick, dashed, rounded corners] (0.841,0.62) rectangle (0.918,0.284);
      \coordinate (L) at (0.731,0.3856);
      \coordinate (C) at (0.25625, 0.7116);
      \coordinate (L2) at ($(C)!1.14!3:(L)$);
      \coordinate (L1) at ($(C)!0.89!-3:(L)$);
      \draw[axis] (0.27,0.428) coordinate (O) -- ($(O)!1.4!(L)$) coordinate(x) node(xline)[below right] {$x$};
      \draw[axis] (O) -- (0.475,0.56) coordinate(y) node(yline)[above] {$y$};
      \draw[thick, dashed] (C) -- (L);
      \draw[thick, dashed] (C) -- (L2);
      \draw[thick, dashed] (C) -- (L1);
    \end{scope}
  \end{tikzpicture}
\end{center}
\caption{Measurement covariance of point landmark}
\label{fig:measurecovar}
\end{figure}



\subsubsection{Landmark Correspondence}
Due to the high ambiguity of the features in \gls{SPL}, one of the difficulties of using Kalman filter for localization is to find the correspondence between the observations and the landmarks. In other word, it is to find the corresponding $z_t$ (in \autoref{eq:observ}), when a landmark $l$ is detected. 

\paragraph{Center Circle}
The only unique landmark currently can be used is the center circle with center line detected. Its correspondence is easily fixed to $z_t = (0,0,0)$, meaning position $(0,0)$ in physical world coordinate and \SI{0}{\degree} in orientation. However, because the center circle is symmetrical, it can not provide the information of the field side, so the robot could be in either one position or its mirrored one. In this case, the side of the current robot position is considered when using center circle to update robot position, then the robot position can be uniquely defined. With the help of unique landmark like center circle, the global localization problem stated in \autoref{sub:problem} can be solved.

On the other hand, when the center circle is detected without the middle line, the robot position can still not be decided, since it can be at any position surrounding the center circle. So in this case, the center circle can only be treated as a point landmark without orientation $z_t = (0,0)$.

%The same problem applies to the penalty area. Even though it is a unique landmark in half field, as said in \autoref{subsub:sensorModelChoice}, it is only treated as a line. So the robot position can be any where parallel along the line. To find a correspondence of the penalty area, we assume there is a virtual line 

\paragraph{Junctions}
For junction features which include ``L'',``T'',``X'' junctions, they are ambiguous features and the detected junctions are represented as point landmark with orientation $l = (x,y,\theta)$. Within the junctions, ``L'' junctions are the most ambiguous ones, with 36 appearances in the field (illustrated in \autoref{fig:ljunctions}). It means one detected ``L'' junction can possibly be matched to 36 ``L'' junctions in the field . `T'', ``X'' junctions are less pervasive, with 14 and 2 appearances respectively. Given the ubiquitous distribution of the junctions in the field, it is hard to find the correspondence between the junctions seen and the junctions in the field. \\
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{fieldLcorners.png}
  \caption{All L junctions in the field, the blue pointer indicates the position of the ``L'' junctions as well as their directions}
  \label{fig:ljunctions}
\end{figure}

In this thesis work, a \textit{nearest neighbor search method with threshold} is used to find the junction correspondences. Assume the robot position is not of high inaccuracy, then the junctions in robot frame can be transformed into physical world frame. Given the junction in physical world frame, the associative correspondence is found by searching the nearest junction in the field. 

Instead of looping through all the possible junctions, the k-d tree algorithm \cite{Bentley1975} is adopted to do the search efficiently. Two k-d trees are constructed, one for ``L'' junctions and one for ``T'' junctions. No k-d tree is needed for ``X'' junction, since the ``X'' junctions can be compared directly with the two in the field. The distance of measure between the junctions are Euclidean distance. 

Once the associative junction is found by nearest distance, the difference in orientation is checked. If the distance to the associative junction is larger than a threshold, or the difference in orientation is too large, the associative junction is simply discarded and this landmark will not contribute to the robot position update. Because in this case, the assumption that current robot position is relatively accurate no longer holds, more than one possible correspondence can be chosen. In other word, \textit{nearest neighbor search with threshold} acts more like a local position optimization to track robot pose rather than global localization. In \autoref{sec:resample}, we will see the discarded landmarks can be utilized for global localization by using multi-hypotheses Kalman filter. The matching result is already shown before in \autoref{fig:txcorner}, in which the orange pointer indicates to which junction the detected junction is corresponding.

\paragraph{Lines}
Lines are the most ambiguous landmarks among all the landmarks in the field, because a single line can be matched to any field line in the field. Due to the high ambiguity, the lines also can not be used for global localization if features are not extracted from them. To find the line correspondence, a similar methodology as the nearest neighbor search is adopted. Each detected line is represented by a vector of two end points $l=(p_{start}, p_{end})$. If the nearest neighbor search is directly applied, the two end points could be matched to two different field lines, as illustrated in \autoref{fig:linematch}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
  \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.6\textwidth]{linematch_crop.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
      \coordinate (start) at (0.133, 0.878);
      \coordinate (end) at  (0.765,0.1852);
      \draw (start) node() [above]{$p_{start}$};	
      \draw (end) node() [below]{$p_{end}$};	
    \end{scope}
  \end{tikzpicture}
\caption{The end points of the detected line (grey) can possibly be matched to two different field lines}
\label{fig:linematch}
\end{figure}

Inspired by the method used by B-Human \cite{Bhuman} to find line correspondence, a \textit{nearest line neighbor search} is implemented. Given a detected line, after projecting it to physical world frame, it is compared with each of the field lines to check if certain criteria are satisfied. Suppose we start with the outer side of the horizontal field line (indicated by red in
\autoref{fig:find_line_correspondence}), the criteria are as follows:
\begin{itemize}
  \item Find the nearest distance from the end points of detected line to the field line, in this case, distance $p_{start}A$ and $p_{end}B$ are checked. They should below a threshold \footnote{The threshold is chosen to be half of the penalty area length defined in \autoref{tab:field}, since it is the largest distance that can distinguish the ambiguity between lines.} respectively, otherwise, this field line is not the correspondence.
  \item Start from each end point of the detected line, make a perpendicular line pointing to the field line, \ie line $p_{start}C$ and $p_{end}D$. If both perpendicular lines have intersections with the field line, the distance of $p_{start}C$ and $p_{end}D$ are checked. Each of them should also below the threshold. Moreover, if either orthogonal line starting from the end point fails to have an intersection with the field line. This field line is regarded as not the correspondence. (For example, $p_{start}$ fails to have an orthogonal intersection with the vertical field line in \autoref{fig:find_line_correspondence}) 
   \item When both of the criteria above are satisfied, the direction of the detected line is checked with the field line. They should have more or less the same direction. And this is done by dot multiplying the two vectors, the one with positive result satisfies.
   \item Only the field line which satisfies all the criteria above is treated as the correspondence. If more than one field line happen to satisfy the criteria, then all of them are discarded due to the ambiguity.
\end{itemize}

\begin{figure}[h!]
  \centering
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.6\textwidth]{linematch_crop.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
      \draw[thick, red] (0, 0.719) coordinate(x) -- (1,0.7197) coordinate(y); 
      %projection
      \draw[thick, dashed] (0.133, 0.878) coordinate(start) -- (0.133,0.7197) coordinate(proja); 
      \draw[thick, dashed] (0.765,0.1852) coordinate(end)-- (0.765, 0.7197)  coordinate(projb); 
      % intersection
      \draw[blue, thick, dashed] (start) -- (0.068,0.7197) coordinate(intera); 
      \draw[blue, thick, dashed] (end) -- (0.912,0.7197) coordinate(interb); 

      \def\ralen{1ex}  % length of the short segment
      \foreach \inter/\first/\last in {proja/start/y, projb/end/y, start/intera/end, end/interb/start}
      {
	\draw let \p1 = ($(\inter)!\ralen!(\first)$), % point along first path
	\p2 = ($(\inter)!\ralen!(\last)$),  % point along second path
	\p3 = ($(\p1)+(\p2)-(\inter)$)      % corner point
	in
	(\p1) -- (\p3) -- (\p2) ;              % path
       %($(\inter)!.5!(\p3)$) node [dot] {};  % center dot
      }

      \draw ($(start)!0.5!(proja)$) node() [right]{$d1$};	
      \draw (proja) node() [below]{$A$};	

      \draw ($(end)!0.5!(projb)$) node() [left]{$d2$};	
      \draw (projb) node() [above]{$B$};	

      \draw ($(start)!0.5!(intera)$) node() [left]{$d3$};	
      \draw (intera) node() [below]{$C$};	

      \draw ($(end)!0.5!(interb)$) node() [right]{$d4$};	
      \draw (interb) node() [above]{$D$};	

      \draw (start) node() [above]{$p_{start}$};	
      \draw (end) node() [below]{$p_{end}$};	

     %\draw () node() {$\rho$};	
     %\draw () node() {$\rho$};	
     %\draw () node() {$\rho$};	
    \end{scope}
      %\draw[dashed] (0.414+0.1, 0.508) coordinate(yy) -- (0.178, 0.512) coordinate(Cy) node()[left] {$y_{r}$};
  \end{tikzpicture}
  \caption{The criteria of determining the line correspondence}
  \label{fig:find_line_correspondence}
\end{figure}


Another important property of finding correspondence of line is that short lines can match to long lines and short lines, but long line can not match to short line. Thus, the long lines and short lines are distinguished, the lines with length exceeding the penalty area width (defined in \autoref{tab:field}) are treated as long lines, the rest are short lines. With this classification, long lines are limited to the four field lines on the border and one field line in the center. The benefit is discernible, not only the process of \textit{nearest line neighbor search} can be speeded up for long lines, but also the threshold in the matching criteria can be enlarged, since the ambiguity is reduced. 

While each line detected from a vertical or horizontal field lines can have a correspondence, the lines detected from the center circle can hardly have. The center circle can be treated as consisting of infinite short lines, and each line can be of different length. The algorithm implemented checks if a short line is in the vicinity of the center circle, if so, the line is simply discarded. Therefore, in this case, localization depend more on center circle detection.


%To cope with this, the same methodology from junction correspondence is taken, namely local position optimization. 


\paragraph{Penalty Area}
As we discussed earlier in \autoref{par:lineLandmark}, detected penalty area can be regarded as a line. We can assume on the field, there is a virtual line which is vertical and goes through the center of each penalty area box. Then the problem becomes the matching of the penalty area with the virtual lines. Unlike the \textit{nearest line neighbor search} for lines, the penalty area matching criterion is simpler. Since there are only two penalty area virtual lines in the field, the nearest distance from the center of the detected penalty area to both virtual lines are calculated. The virtual line with shorter distance becomes the correspondence. The matching result for both detected lines and penalty area is illustrated in \autoref{fig:linematch}.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.7\textwidth]{line_penaltymatching_crop.png}
\end{center}
\caption{Matching result of line and penalty area. (The gray box indicates the detected penalty area. The line in blue indicate it is matched with the field line in red. The line in grey indicate it fails to find a correspondence.)}
\label{fig:linematch}
\end{figure}

