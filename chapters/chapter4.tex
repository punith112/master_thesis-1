\chapter{Localization Design\label{cha:chapter4}}


As stated in \autoref{sub:kalman}, Kalman filter works by combining prediction step and measurement update to give an optimal estimate of the current state variable. In this paper, odometry information from motion module is used to perform the prediction step. From odometry, the information of how much distance the robot moved relative to the pose of last time stamp is provided, thus an estimate of the current position can be established. Likewise, an estimate from the measurement update is performed based on what the robot has observed, or simply put, the vision results from the cameras. In the end, Kalman filter update the state variable by a weighted average of the estimates.

The choice of motion model and observation model is fundamental for both Kalman filter's prediction step and measurement update. The more precise and comprehensive the model describes the system, the more accurate the estimate of the state will be. 

\section{Motion Update}\label{sec:Motion Update}
\subsection{Motion Model}\label{sub:Motion Model}
In general, motion models can be categorized by two kinds: velocity motion model and odometry motion model. Practical experience suggests that odometry, while still erroneous, is usually more accurate than velocity. Both suffer from drift and slippage, but velocity additionally suffers from the mismatch between the actual motion controllers and its mathematical model \cite{thrun2005probabilistic}. It is especially true for humanoid robot like NAO, whose moving velocity is difficult to model. 

Since the transformation between the coordinate used internally by the odometry measurement and the physical world coordinate is unknown, the internal odometry measurement in this motion model is relative. 
To be specific, in the time interval $(t-1, t]$, the robot moves from a position $x_{t-1}$ to position $x_t$, and meanwhile the odometry reports us a related movement from $\bar{x}_{t-1} = (\bar{x}, \bar{y}, \bar{\theta})$ to $\bar{x}_{t} = (\bar{x}', \bar{y}', \bar{\theta}')$. The bar here indicates that these are odometry measurements. We use the relative difference of $\bar{x}_{t-1}$ and $\bar{x}_{t}$ as an estimation of the difference between the true position $x_{t-1} = (x, y, \theta)$ and $x_t = (x', y', \theta')$. Therefore, the odometry information $u_t$ can be given by the pair:

\begin{equation}\label{eq:ut}
u_t = \begin{pmatrix}
\bar{x}_{t-1}\\ 
\bar{x}_{t}
\end{pmatrix}
\end{equation}

In the following, let \autoref{eq:rot} denote the rotation matrix with angle $\alpha$.
\begin{equation}\label{eq:rot}
\Omega(\alpha) = \begin{bmatrix}
\cos{\alpha} &  -\sin{\alpha} \\
\sin{\alpha} &  \cos{\alpha}
\end{bmatrix}
\end{equation}

In our motion model, to obtain the relative odometry change, $u_t$ can be treated as a translation and then followed by a rotation. \autoref{fig:motion_model} demonstrates the decomposition of the odometry measurement. $\delta_{trans}$ is the translation and $\delta_{rot}$ is the rotation after translation. Both the translation and the rotation are considered under the coordinate $O$, which is relative to $\bar{x}_{t-1}$. The relative translation and rotation is calculated using \autoref{alg:substract poses}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.6\textwidth]{motion_modela.png}
  \caption{Relative odometry change. Translation $\delta_{trans}$, rotation $\delta_{rot}$ are considered in coordinate system $O$}
  \label{fig:motion_model}
\end{figure}

\begin{algorithm}                      
  \caption{subtract\_poses ($\bar{x}_{t-1}, \bar{x}_{t}$)}         % give the algorithm a caption
\label{alg:substract poses}                           
\begin{algorithmic}[1]  
\State $\begin{bmatrix}
\delta_{x}\\ 
\delta_{y}
\end{bmatrix}
=
\Omega(-\bar{\theta})
%\begin{bmatrix}
%\cos{\bar{\theta}} & \sin{\bar{\theta}}\\ 
%-\sin{\bar{\theta}} & \cos{\bar{\theta}}
%\end{bmatrix}\cdot 
\begin{bmatrix}
\bar{x}' - \bar{x}\\ 
\bar{y}' - \bar{y}
\end{bmatrix}$

\State $\delta_{rot} = \bar{\theta}' - \bar{\theta}$
\State $return (\delta_{x}, \delta_{y}, \delta_{rot})$

\end{algorithmic}
\end{algorithm}

Once the estimated translation $\delta_{trans}$ and rotation $\delta_{rot}$ relative to pose $x_{t-1}$ is obtained, it can be applied to update the robot position in the physical world coordinate. The full motion update using odometry measurement is shown in \autoref{alg:motion_update}.

\newcommand{\transfunc}{odometry\_motion\_update}

\begin{algorithm}                      
  \caption{\transfunc ($x_{t-1}$, $u_t$)}         % give the algorithm a caption
\label{alg:motion_update}                           
\begin{algorithmic}[1]                    
  %\State $\delta_{x}, \delta_{y}, \theta_{r} \gets \Call{sub_pose}{\bar{x}_{t-1}, \bar{x}_{t}}$
  \State $(\delta_{x}, \delta_{y}, \delta_{rot}) = substract\_poses (\bar{x}_{t-1}, \bar{x}_{t})$
  \State $
\begin{bmatrix}
  x'\\ 
  y'\\ 
\end{bmatrix}
= 
\Omega(\theta)
% \begin{bmatrix}
%   \cos\theta  & -\sin\theta \\ 
%   \sin\theta &  \cos\theta  
% \end{bmatrix}
\cdot 
\begin{bmatrix}
  \delta_{x}\\ 
  \delta_{y} 
\end{bmatrix} +
\begin{bmatrix}
  x\\ 
  y 
\end{bmatrix}\
$
\State $\theta' = \theta + \theta_{rot}$ 
  %\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $return [x', y', \theta']^\top$
\end{algorithmic}
\end{algorithm}

The function \textit{\transfunc} in \autoref{alg:motion_update} corresponds to the transition function $f(x_{k-1}, u_{k})$ described in \gls{EKF} algorithm in \autoref{tab:ekf}. Then the related Jacobian Matrix $F_t$ can be calculated by \autoref{eq:jacobianF}.
\begin{equation}\label{eq:jacobianF} 
F_t = 
\begin{bmatrix}
1 & 0 & -sin(\theta)\cdot \delta_x - cos(\theta)\cdot\delta_y\\ 
0 &  1& cos(\theta)\cdot \delta_x  - sin(\theta)\cdot\delta_y\\ 
0 & 0 & 1
\end{bmatrix} 
\end{equation}

\subsection{Process Noise Model}
\label{sub:Process Noise Model}

The process noise covariance matrix $Q_t$ is modeled to be positive proportional to the absolute change of odometry $(|\delta_{x}|, |\delta_{y}|, |\theta_{r}|)$. Since the more the robot moves in a time interval, the more it suffers from unpredictable noises like slippage and drift. \autoref{eq:Qt} is proposed to calculate process noise covariance matrix.
\begin{equation}\label{eq:Qt} 
Q_t = 
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix} 
\cdot
Sc^2 
\cdot
\begin{bmatrix}
|\delta_x| & 0 & 0\\ 
0 &  |\delta_y|& 0\\ 
0 & 0 & |\delta_{rot}|
\end{bmatrix}^\top
\end{equation}

\begin{equation}\label{eq:Sc} 
Sc = \begin{bmatrix}
0.8 & 0.2 & 0.2\\ 
0.2 &  0.8 & 0.2\\ 
0.2 & 0.2 & 0.8
\end{bmatrix}
\end{equation}

$Sc$ is a scaling matrix and $Sc^2$ is square elementwise. It intuitively means, when robot walk \SI{1}{\meter} in x or y direction, it may have an error range in $\pm$ \SI{0.8}{\meter}. When it rotates \SI{1}{\radian}, it may have an error of $\pm$ \SI{0.8}{\radian} in orientation. The value of $Sc$ depends on the accuracy of odometry measurement from the motion module, and a better tuning of the values can be argued.

With all the essential elements for motion model ready, we plug in the formula from \autoref{tab:ekf} and perform the \gls{EKF} prediction step for the localization algorithm. The only thing we haven't touched yet is the initial value of $x_{t-1}$ and $P_{t-1}$, it will be discussed in ?(to be determined).  



\section{Sensor Update}\label{sec:sensor update}
The robot can also measure its position by sensing the surrounding environment. In the case of NAO robot, the measurement is made by observing the environment from its two cameras. The images from the cameras are preprocessed by the DAInamite's vision module to extract the landmarks as shown in \autoref{fig:perception}. During the work of this thesis, two sensor update models are proposed to accomplish the localization task. One is based on an optimization approach and another is based on features. We will discuss both models in this section.

\subsection{Optimization Based Model}\label{sec:optimization update} 
In the early phase of the thesis, the only unique landmark can be detected by vision module is the yellow goal posts, however, from 2015 the yellow goal post will be replaced by white goal post according to the rule \cite{Committee2013}. Then the only landmark available is the white line in the field. Field lines are of high ambiguity, in the case of feature based Kalman filter localization, a wrong matching of the landmarks could lead to the divergence of the tracking which can hardly be recovered. Thus, a robust and accurate algorithm for sensor update is implemented taking the reference from Lauer \cite{Lauer2006}. The algorithm is based on Rprop \cite{Riedmiller1993} optimization method, which uses only the line points from the detected lines, to find the robot position which matches the field map best. 



% \subsubsection{Image Frame to Robot Frame}
% 
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=.8\textwidth]{camera-matrix.png}
%   \caption{coordinate of the bottom camera and the robot local coordinate}
%   \label{fig:motion_model}
% \end{figure}


\subsection{Feature Based Model}\label{sec:feature update} 
Although the optimization method based localization outperforms the particle filter in terms of accuracy, it is not computational efficient to run on the robot. During the preparation of German Open 2015, DAInamite team developed more feature detections like center circle detection, penalty area detection and ``L'' junction detection. Provided the richness of features, the development of feature based sensor update for localization is motivated.  

\subsubsection{``T'' and ``X'' Junction Detection}
In the \gls{SPL} soccer field, there are 36 ``L'' junctions, 14 ``T'' junctions, 2 ``X'' junctions \footnote{The detection of ``L'' consider both side of the field line as well as the line direction, ``T''and ``X'' junctions do not consider line side and direction. The counting includes ``L'', ``T'', ``X'' junctions at center circle}. Compared to ``L'' junction, ``T'' and ``X'' junctions are much stronger landmarks. Especially the ``T'' junction at the penalty area, it can significantly help the robot localize itself in front of the goal.
Both the ``T'' and ``X'' junctions are detected in the robot frame instead of the image plane, since in the image plane the perpendicularity of the lines can not be easily observed. When the detected lines are projected to the robot frame, every two lines are compared according to their angles. If the differences of the angles are close to \SI{90}{\degree}, then the intersection point of the two lines are computed assuming the lines have infinite length. After the intersection is calculated, it should be satisfied that the intersection lies on at least one of the line segments. The distance from line end points to the intersections are further checked if they satisfy the threshold according to T or X geometry. The pseudocode of the TX junction detection is illustrated in \autoref{alg:txdetection}.

\begin{algorithm}                      
  \caption{TX\_junction\_detection ()}         % give the algorithm a caption
\label{alg:txdetection}                           
\begin{algorithmic}[1]                    
  \For{each $Line(i)$}
  	\For{each $Line(j)$}
		\If {\Call{AngleDifference}{$Line(i)$, $Line(j)$} $\approx$ \SI{90}{\degree}}
		\State $intersection \gets$ \Call{Intersection}{$Line(i)$, $Line(j)$}
		    \If {$intersection$ lies on both $LineSegment(i)$ and $LineSegment(j)$}
			\If {\Call {CheckDistance}{$LineEndPoints(i)$, $LineEndPoints(j)$, $intersection$} within ThresholdX}
				\State $LineSegment(i)$ and $LineSegment(j)$ is a X junction
			\EndIf
		    \ElsIf{$intersection$ lies on either $LineSegment(i)$ or  $LineSegment(j)$}
			\If {\Call {CheckDistance}{$LineEndPoints(i)$, $LineEndPoints(j)$, $intersection$} within ThresholdT}
				\State $LineSegment(i)$ and $LineSegment(j)$ is a T junction
			\EndIf
		    \EndIf
		\EndIf
	\EndFor
  \EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Measurement Model Choice}\label{subsub:sensorModelChoice}
Given the various kinds of features, different sensor model can be distinguished for different features. In general, they can be divided into point Landmarks and line Landmarks. Assume the robot position $x_t = (p_x, p_y, p_{\theta})$, and the landmark represented as $l$. The objective is to describe the expected measurement $\bar{z}$ by the observation function $h(x_t)$ and derive the Jacobian matrix $H$.
\paragraph{Point Landmark without orientation}\label{par:pointLandmark}
Under the Cartesian coordinate system in robot frame. The expected point landmark can be described as $\bar{z} = (z_x, z_y)^\top$ (illustrated in \autoref{fig:observation_euclidean}) and its corresponding landmark in physical world frame $l = (l_x, l_y)^\top$. The observation function $h(x_t)$ can be described in \autoref{eq:hx}, which transforms the landmark from physical world frame to the robot local frame. The Jacobian matrix $H$ is the partial derivative of $h(x_t)$ at $x_t$, as illustrated in \autoref{eq:jH}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=.8\textwidth]{observation_euclidean.png}
  \caption{Observation given in Cartesian coordinates \cite{Tasse2013}}
  \label{fig:observation_euclidean}
\end{figure}

\begin{equation}\label{eq:hx}
  \bar{z} = \begin{pmatrix}
\bar{z}_{x}\\ 
\bar{z}_{y}
\end{pmatrix}
= 
h(x, l)
=
\Omega(-p_{\theta})
\cdot
\begin{bmatrix}
 \begin{pmatrix}
l_{x}\\ 
l_{y}
\end{pmatrix}
-
\begin{pmatrix}
p_{x}\\ 
p_{y}
\end{pmatrix}
\end{bmatrix}
\end{equation}

\begin{equation}\label{eq:jH}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  -\cos{p_{\theta}} & -\sin{p_{\theta}} & -(l_x-p_x)\sin{p_{\theta}} + (l_y-p_y)\cos{p_{\theta}} \\ 
  \sin{p_{\theta}} & -\cos{p_{\theta}} & -(l_x-p_x)\cos{p_{\theta}} - (l_y-p_y)\sin{p_{\theta}} 
\end{bmatrix}
\end{equation}
The feature which follows this model is center circle without the center line being detected.

\paragraph{Point Landmark with Orientation}\label{par:pointLandmark}
When the point landmark has not only position but also orientation, it can be described as $l = (l_x, l_y, l_{\theta})^\top$ in the physical world frame, and the expected point landmark can be described as $\bar{z} = (z_x, z_y, z_{\theta})^\top$.  With addition of orientation, the observation function $h(x_t)$ can be described in \autoref{eq:hx2} and the Jacobian matrix $H$ in \autoref{eq:jH2}.

\begin{equation}\label{eq:hx2}
  \bar{z} = \begin{pmatrix}
\bar{z}_{x}\\ 
\bar{z}_{y} \\
\bar{z}_{\theta} 
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}

\Omega(-p_{\theta})
\cdot
\begin{bmatrix}
 \begin{pmatrix}
l_{x}\\ 
l_{y}
\end{pmatrix}
-
\begin{pmatrix}
p_{x}\\ 
p_{y}
\end{pmatrix}
\end{bmatrix} \\
l_{\theta} - p_{\theta}
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jH2}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  -\cos{p_{\theta}} & -\sin{p_{\theta}} & -(l_x-p_x)\sin{p_{\theta}} + (l_y-p_y)\cos{p_{\theta}} \\ 
  \sin{p_{\theta}} & -\cos{p_{\theta}} & -(l_x-p_x)\cos{p_{\theta}} - (l_y-p_y)\sin{p_{\theta}}  \\
  0 & 0 & -1
\end{bmatrix}
\end{equation}
The landmarks which comply with is measurement model are ``L'',``T'',``X'' junctions, and center circle with center line detected.

\paragraph{Line Landmark}\label{par:lineLandmark}
Line is a special landmark, it needs two end points to describe it in Cartesian space. However, with only two end points of the line, it is very hard to find the corresponding points in the field map. For example, a short line can be matched to any segment within a long line. Therefore, to counter this ambiguity and extract the intrinsic property from lines, the representation from Hough transform is used where a line can be represented as ($\rho$, $\theta$), as shown in \autoref{fig:hough}. 
\begin{figure}[h!]
  \centering
 \begin{tikzpicture}[
     scale=5,
     axis/.style={very thick, ->, >=stealth'},
     important line/.style={thick},
     dashed line/.style={dashed, thin},
     pile/.style={thick, ->, >=stealth', shorten <=2pt, shorten
     >=2pt},
     every node/.style={color=black}
     ]
     % axis
     \draw[axis] (0,0) coordinate (O) -- (1.1,0) coordinate(x) node(xline)[right]
         {$x$};
     \draw[axis] (0,0) -- (0,1.1) coordinate(y) node(yline)[left] {$y$};
     % Lines
 
     \draw[important line] (0.1,0.942) coordinate (A) -- (0.832,0.519) coordinate (B) node[right, text width=5em] {\textit{Line}};
 
     \draw[dashed] (O) -- (0.433, 0.75) coordinate(C);
     \draw (0.216, 0.5) node() {$\rho$};	
     \draw pic[ draw=orange, <->, angle eccentricity=1.2, angle radius=1cm]
     {angle=x--O--C};

     \draw (0.2,0.1) node()[right] {$\theta$};
     \def\ralen{.5ex}  % length of the short segment
     \foreach \inter/\first/\last in {C/O/B}
     {
       \draw let \p1 = ($(\inter)!\ralen!(\first)$), % point along first path
       \p2 = ($(\inter)!\ralen!(\last)$),  % point along second path
       \p3 = ($(\p1)+(\p2)-(\inter)$)      % corner point
       in
       (\p1) -- (\p3) -- (\p2) ;              % path
       %($(\inter)!.5!(\p3)$) node [dot] {};  % center dot
     }
   \end{tikzpicture}
   \caption{Line represented in Hough space}
   \label{fig:hough}
 \end{figure}
While Hough representation discards the length information of the line, it keeps the direction and distance information from the line intact. It means, for example, when a vertical line is matched, the robot's orientation can be determined and it will also be sure about its position in $x$ axis, but keep uncertain about its position in $y$ axis until a horizontal line is matched.

Using Hough representation, the expected line landmark can be described as $\bar{z} = (z_{\rho}, z_{\theta})^\top$, and depending on whether the corresponding line in the field map is vertical or horizontal, different measurement model should be adjusted. If knowing the matched line is horizontal, the line landmark in physical world frame can be describe using only $y$ axis value $l = l_{y}$. Then the observation function $h(x_t)$ can be described in \autoref{eq:hxhorizon} and the Jacobian matrix $H$ in \autoref{eq:jHhorizon}. \\
\begin{equation}\label{eq:hxhorizon}
  \bar{z} = \begin{pmatrix}
\bar{z}_{\rho}\\ 
\bar{z}_{\theta}
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}
|l_{y}-p_{y}| \\
atan2(\cos{p_{\theta}\cdot (l_y-p_y)}, \sin{p_{\theta}}\cdot (l_y-p_y))
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jHhorizon}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  0 & \frac{l_{y}-p_{y}}{|l_{y}-p_{y}|} & 0 \\ 
  0 & 0 & -1 
\end{bmatrix}
\end{equation}

Similar for vertical lines, the line landmark in physical world frame can be describe using only $x$ axis value $l = l_{x}$. The observation function $h(x_t)$ can be described in \autoref{eq:hxvertical} and the Jacobian matrix $H$ in \autoref{eq:jHvertical}. \\
\begin{equation}\label{eq:hxvertical}
  \bar{z} = \begin{pmatrix}
\bar{z}_{\rho}\\ 
\bar{z}_{\theta}
\end{pmatrix}
= 
h(x, l)
=
\begin{pmatrix}
|l_{x}-p_{x}| \\
atan2(\cos{p_{\theta}\cdot (l_x-p_x)}, -\sin{p_{\theta}}\cdot (l_x-p_x))
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jHvertical}
H =
\frac{\partial h(x_t,l))}{\partial x_t}
=
\begin{bmatrix}
  \frac{l_{x}-p_{x}}{|l_{x}-p_{x}|} & 0 & 0\\ 
  0 & 0 & -1 
\end{bmatrix}
\end{equation}

The features fall into this measurement model is the detected lines as well as the penalty area. Because the penalty area detection basically is just the detection of two parallel lines which satisfies certain criteria. The penalty area detection result is given in the form of Hough representation, thus it is also treated as a line.

\subsubsection{Measurement Noise Model}
\label{sub:Measurement Noise Model}
The measurement noise indicates how accurate the measurement made for each feature is, and by assuming the noise distribution is Gaussian, the measurement noise covariance is represent by the term $R_{t}$ in \autoref{tab:ekf}. The Measurement noise in case of NAO robot in \gls{SPL} game can mainly come from:
\begin{itemize}
  \item inaccuracy of the camera matrix
  \item blur of the image due to motion
\end{itemize}
Since all the landmarks are measured in robot frame, the further the landmarks from the robot (the origin of robot frame coordinate), the larger the measurement noise will be. Because the error introduced by the camera matrix will be magnified when the detected landmarks are projected from image plane to robot frame. This phenomenon is demonstrated in \autoref{fig:landmarkImagePlane} and \autoref{fig:landmarkRobotFrame}. For example, the line (both sides) which being detected from the penalty area in \autoref{fig:landmarkImagePlane} become two lines with big gap (shown in \autoref{fig:landmarkRobotFrame}) when projected to the robot frame. Both lines suffered significant error in distance and angle.\\

\begin{figure}[h!]
  \centering
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.8\textwidth]{measure_camera.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[red,ultra thick, dashed, rounded corners] (0.179,0.59) rectangle (0.459,0.53);
    \end{scope}
\end{tikzpicture}
  \caption{Landmarks in image plane. (The red box indicates the lines which detected from the front line of the penalty area)}
  \label{fig:landmarkImagePlane}
\end{figure}


\begin{figure}[h!]
  \centering
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{measure_err.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[red,ultra thick, dashed, rounded corners] (0.841,0.62) rectangle (0.918,0.284);
    \end{scope}
\end{tikzpicture}
  \caption{Landmarks in robot frame. (Grey lines indicate the detected lines after projection, the red box indicates the lines which detected from the front line of the penalty area)}
  \label{fig:landmarkRobotFrame}
\end{figure}

A straight forward way to describe this phenomenon is to model the noise proportional to the distance of the landmark in robot frame. However, the problem with this model is that it is hard to choose a physically meaningful coefficient for the linear function. Therefore, another similar but more suitable noise model which is used by team B-Human is adopted \cite{Bhuman}. (figure needed to describe)


\subsubsection{Landmark Correspondence}
Due to the high ambiguity of the features in \gls{SPL}, one of the difficulties of using Kalman filter for localization is to find the correspondence between the observations and the landmarks. In other word, it is to find the corresponding $z_t$ (in \autoref{eq:observ}), when a landmark $l$ is detected. The only unique landmark currently detected are the center circle and the penalty area. However, these unique features can not provide the information of the field side, so the robot could be in either one position or its mirrored one. 


