\chapter{Design\label{cha:chapter4}}
The goal of this paper is to parallelize the segment-wise \gls{BPE} stage of the CCSDS 122.0-B-1 standard. Each segment could be executed mutually exclusive of the other. The \gls{BPE} loop nest could be visualized wherein the DWT processed pixels are partitioned into NUM\_SEGMENTS segments. The one-dimensional loop nest is mapped onto a one-dimensional thread grid. Each thread processes one segment of the encoder as illustrated in \autoref{fig:des:tmap}. Each segment comprises of $S$ blocks where $S=16$ is chosen as a fixed parameter for this architecture. The reasons for this configuration shall be explained subsequently.
\\
\begin{verbatim}
for (int i=0; i<NUM_SEGMENTS; ++i)
{
    // initialize encoder/headers 
    // encode segment headers 
    // encode quantized DC coefficients 
    // encode additional bit planes of DC coefficients 
    // specifying the AC bit depth in each block 
    // encode bit planes 
    // finish segment (flush buffers) 
}
\end{verbatim}
\begin{figure}[tb]
  \centering
  \includegraphics[width=.7\textwidth]{thread_mapping}
  \caption{Thread mapping for the segment-wise BPE on GPU}
  \label{fig:des:tmap}
\end{figure}

\section{Conversion of \texttt{C++} Refererence code to \gls{CUDA} C}\label{sec:convrefcud}
\section{Thread mapping}\label{sec:tmap}
\gls{CUDA} allows to split the work in terms of threads, wherein a group of threads constitutes a thread block and further, a group of thread blocks forms a thread grid. The choice for these thread configurations is directly dependent on the loop index which is in this case the number of segments NUM\_SEGMENTS. Also, the NVDIA architecture influences the potential size of the thread block. The threads within a thread block are further divided into groups of 32 threads called \textit{warps} which are eventually scheduled by the \textit{warp scheduler} onto the \gls{SM}. The NVIDIA GK104 \gls{SM} Kepler architecture has 4 warp schedulers as shown in \autoref{fig:des:wsched} to pick 4 active warps per clock cycle and dispatches them to the execution units. Hence it is always customary for each thread block to have at least 4 warps to ensure peak utilization of the \gls{SM}. Hence the number of threads within a thread block is chosen to be $32\times 4=128$. Consequently, the number of thread blocks would be as follows.
\begin{figure}[tb]
  \centering
  \includegraphics[width=.99\textwidth]{warp_scheduler}
  \caption{Illustration of the warp-scheduler in a GK104 \gls{SM}}
  \label{fig:des:wsched}
\end{figure}

\begin{verbatim}
int blocksPerGrid = (NUM_SEGMENTS/128) + 1;
\end{verbatim}

\section{Choice of encoder parameters}\label{sec:params}
The CCSDS 122.0-B-1 standard is configured with quality parameters to operate in lossless mode in order to analyze worst case behavior and to facilitate perfect reconstruction of the input images. \autoref{fig:des:compeff} shows the impact of the segment size $S$ on the compression efficiency in lossless mode. Lesser the bits per pixel consumed by the compressor, better the compression efficiency. For all the 4 test images under consideration, it can be noted that the value of $S$ has minimal or no impact on the compression efficiency. Since the encoder is ported onto a \gls{GPGPU} and the \gls{BPE} inherently has a lot of loop nests with loop index $S$, it is intuitive to reduce the value of $S$ in order to reduce the branch latency considering the fact that \glspl{GPU} perform poorly with branches due to branch divergence within the warps. Moreover, lesser the number of blocks within a segment, greater will be the total number of segments to process in the \gls{BPE} stage. Therefore, minimizing the value of $S$ results in achieving maximum possible value for NUM\_SEGMENTS and thereby increasing the degree of parallelism. Hence, the segment size $S$ is set to the least possible value of $16$.
\begin{figure}[tb]
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{coastal_b4}
    \caption{coastal\_b4}
    \label{fig:parameters:coastal_b4}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{ice_2kb4}
    \caption{ice\_2kb4}
    \label{fig:parameters:ice_2kb4}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{foc}
    \caption{foc}
    \label{fig:parameters:foc}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{sar16bit}
    \caption{sar}
    \label{fig:parameters:sar}
  \end{subfigure}\\
\caption{Impact of the segment size $S$ on the compression efficiency}
\label{fig:des:compeff}
\end{figure}

\section{Concatenating the output bit-stream buffer}\label{sec:concat}
Since each segment is processed independently by different threads, the individual bit-streams generated by each thread have to be combined to produce the final output bit-stream buffer. Moreover, since dynamic allocation of the memory is impossible in \gls{GPU} address space unlike the reference \gls{CPU} implementation, the buffers have to be statically allocated up-front. In order to determine the size of the buffers, it is safe to assume that the compressed output bit-stream size shall not exceed the input. The input buffer size for each segment is $\unit[S\times 64\times 4]{byte}$, as each segment contains $S$ blocks and each block consists of $64$ coefficients wherein each wavelet coefficient does not exceed \unit[20]{bit}. Hence the output bit-stream buffer size for each segment is also assumed to not exceed this limit even in the worst-case scenario of a high-entropy image (e.\,g. noise image). This concatenation process is performed on the host side after the CUDA kernel has completed its execution. \autoref{fig:des:bitstr} illustrates the process of the generation of the output bit-stream.
\begin{figure}[tb]
  \centering
  \includegraphics[width=.6\textwidth]{bitstream_buffer}
  \caption{Output bitstream generation}
  \label{fig:des:bitstr}
\end{figure}

\section{Optimizations}
%\subsubsection{Memory transfers from host to GPU}
%The memory transfers from CPU address space and GPU address space are always costly. In order to reduce this transfer time, ``pinned host memory" is used so that the memory pages in the host are not paged out or swapped out by the host OS. Following CUDA APIs ensures non-pageable host memory to facilitate faster host to GPU memory transfers.
%\begin{verbatim}
%cudaHostAlloc((void **)&blk, sizeof(Block) * blockCount, cudaHostAllocWriteCombined);
%cudaMemcpyAsync(blk_gpu, blk, sizeof(Block) * blockCount, cudaMemcpyHostToDevice, 0);
%\end{verbatim}

\subsection{L1 Cache configuration}
The NVIDIA GK104 \gls{SM} Kepler architecture offers a \unit[64]{kbyte} unified memory subsystem useable as shared memory and also as L1 cache. In general, shared memory is configured for \unit[48]{kbyte} and the remaining \unit[16]{kbyte} behaves as L1 cache. By virtue of a lot of local variables in the \gls{BPE} encoder, it is preferable to have a bigger L1 cache rather than the shared memory. Also, due to the fact that the input data set could not fit onto the shared memory, it was decided to use the bulk of the faster memory subsystem to act as L1 cache. The following CUDA API allows the programmer to set the preference for a \unit[48]{kbyte} L1 cache.
\begin{verbatim}
// Prefer L1 cache of 48 kbyte instead of 16 kbyte
cudaFuncSetCacheConfig(encode, cudaFuncCachePreferL1);
\end{verbatim}

\subsection{Reduced Global memory accesses}
The accesses to the global memory of the GPU is always expensive and keeping this in mind, repeated accesses to the same global memory variables were avoided by fetching them only once and efficiently rearranging the dependent computations.

\subsection{Miscellaneous optimizations}
\begin{itemize}
  \item Branch-free implementation for $\lceil{\log_2 x}\rceil$ was used in place of typical CPU implementations.
  \item The \gls{VLC} encoding for all the stages were performed using a single instance of \textit{vlc\_encoder} object to avoid multiple creations of an object.
\end{itemize}

