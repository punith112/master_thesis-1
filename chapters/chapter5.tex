\chapter{Multi-Hypotheses\label{cha:chapter5}}
While Kalman filter can only deal with uni-modal probability distribution, it does not suffice to handle the ambiguous landmarks that occur in robot's observation, which requires a multi-modal distribution. Therefore, a multi-hypotheses Kalman filter is adapted to handle the ambiguous situations by describing the robot position in the field by a Gaussian sum distribution.
The problem of multi-hypotheses Kalman filter lies mainly in the domain of when and where to add new hypothesis into the Gaussian sum distribution, as well as when to pruning or merge certain hypotheses to restrain the number of hypotheses within a limit, so it does not consume too much of the computation resource.


\section{Model Weight}
As illustrated in \autoref{sub:mmkalman}, each normal probability distribution from the Gaussian sum distribution is represented as \autoref{eq:mmkalman}. In addition to uni-modal Gaussian distribution, the weight $\alpha_i$ has to be determined for each Gaussian distribution model $i$ in the Gaussian sum distribution. The weight, at the same time, illustrates the quality of the state hypothesis. Possible update method like \autoref{eq:weightupdate} has been proposed in ~\cite{alspach1972nonlinear}. However, this update method is only suitable for landmarks with known correspondence. If the correspondence is unknown or ambiguous, like in our case, the ``T'' junctions, the expected observation $z\widehat{}_i$ can not be determined. 

In this thesis work, a voting buffer is used to measure the weight of each model. The voting buffer is constructed by a \gls{FIFO} circular buffer of size $60$. The basic idea is to vote $1$ to the buffer when the observed landmark is matched, and $0$ when not matched. The matching of landmark correspondence is using the nearest neighbor algorithm with threshold discussed in \autoref{subsub:landmarkco}. 
The advantage of using a circular buffer is that, after $60$ observed landmarks, regardless matched or not, the old voting in the buffer will be flushed and will not be counted. In other words, it tries to approximate the current robot state by keeping the memory of the most recent history. The model weight is determined by the average value of the voting buffer. Therefore, the more matches with landmarks the Kalman model makes, the higher weight it will have. Unlike the implementation of particle filter that the total number of particles are fixed, in multi-hypothesis Kalman filters, the number of hypothesis model is dynamic. The adjustment made here is that the weights of all the Gaussian models do not add up to 1, instead they each behaves independently. So the weights of the existing hypothesis models will not be affected by the newly generated ones.

As discussed in \autoref{subsub:landmarkco}, some landmarks are unique landmarks, \ie center circle, and some are ambiguous landmarks, \ie junctions and lines. To depict the quality of the robot localization state, the vote should depend only on the globally unique landmarks instead of ambiguous landmarks. Assume a scenario, where the robot is not moving and constantly observing a ``T'' junction, and the ``T'' junction is perfectly matched with its nearest neighbor in the field. In this case, the state of the robot position is local optimal. The weight of the model will keep increasing and reach 1, but this weight is not truly describing the quality of localization state in global. 

To better express the localization state globally, in the localization algorithm, landmarks are classified differently to update the weight. In observation, center circle with/without orientation and penalty area are treated as unique landmarks, they can directly vote 1 to the voting buffer when the threshold requirement in the nearest neighbor algorithm is satisfied, or vice versa. 

On the other hand, ``L'', ``T'', ``X''junctions are classified as non-unique landmarks, the voting buffer can be voted by 1, only when at least two junctions are matched, and in addition they must each belong to different junctions in the field. The junctions under consideration do not distinguish ``L'', ``T'' or ``X'', and they can come from different observation frames, as long as there is no failure match (beyond threshold) in between. By using the combination of the different observations of junctions, we assume it represents a global unique landmark, thus help to overcome the local optimal problem. While the requirement of voting positive using junctions is high, the condition of voting 0 is the same as the unique landmarks, that is when a failure match happens for the junction, a 0 will be voted to the buffer.  

For the observed line landmarks, they are currently not contributing to the voting buffer. To utilize the lines, certain feature structures needed to be extracted from them, in order to use the same strategy above to update the voting buffer. However, the extraction of other features out of the lines is not a easy task, further work may be done in future research. 
%summing up the values in the voting buffer and calculate the average.


\section{Landmark Based Resampling}\label{sec:resample}

Similar to the augmented particle filter's \cite{thrun2005probabilistic} sensor reseting step to recover the situation of robot gets kidnapped, multi-hypotheses Kalman filter based its sensor reseting on landmarks to recovery from position tracking failure. Assume uni-modal kalman filter for position tracking, the filter can lose track of the robot position when the robot is hit by another robot, which result in an error in robot's orientation, or the robot is kidnapped by a referee during manual replacement, etc. Augmented particle filter initiates the sensor resetting step to generate random particles when the fluctuation of the weights from the particles is high, multi-hypotheses Kalman filter designed in this thesis work will start resampling when an observed landmark fail to match the one on the field. Here, a correspondence match failure is regarded as a signal for potential lose of position tracking. 

The resampling is achieved by calculating all the possible poses of the robot with respect to the observation of the landmark. The landmarks which can trigger resampling are the landmarks with orientation, \ie center circle with center line and all kinds of junctions. The landmarks without orientation like lines, center circle without center line and penalty area can generate infinite possible robot positions, therefore currently they are not used for resampling. The problem of calculating the possible poses can be formulated as follows:\\
Given an observed landmark $l_R = (x_R, y_R, \theta_{R})$ in robot frame, and assume its correspondent landmark $l_G = (x_G, y_G, \theta_{G})$ in physical world frame, the corresponding robot pose in physical world frame is what needed to be calculated. The calculation concerns mainly with the coordinate frame transformation which can be described by homogeneous transform matrix. 

A homogeneous transform matrix in our context is a $3 \times 3$ matrix. It is structured by a $2 \times 2$ rotation matrix $R_B^A$ and a $2 \times 1$ translation matrix  $P_B^A$ denoted in \autoref{eq:homomat}.
\begin{equation}
\label{eq:homomat}
T_B^A =
\begin{bmatrix}
R_B^A & P_B^A \\
\bf{0}   & 1 
\end{bmatrix}
\end{equation}
The sub and super-script in $R_B^A$ indicate it is the rotation of frame $B$ relative to frame $A$. Translation matrix  $P_B^A$ denotes the translation of the origin of frame $B$ in frame $A$.
In the following equations, we represent the robot pose in physical world frame in the form of  homogeneous transform matrix as $T_R^G$, landmark in robot frame as  $T_L^R$, landmark in physical world frame as $T_L^G$.
According to \textit{Composition Rule for Homogeneous Transformations}, the transformation between $T_R^G$, $T_L^R$ and $T_L^G$ can be described in \autoref{eq:trans}.
\begin{equation}
\label{eq:trans}
T_R^G \cdot T_L^R=T_L^G
\end{equation}

To obtain $T_R^G$, we multiply $(T_L^R)^{-1}$ from the right for both side of the equation, then obtain \autoref{eq:reverseTrans}.

\begin{equation}
\label{eq:reverseTrans}
	T_R^G=T_L^G\cdot (T_L^R)^{-1} 
\end{equation}

Expand the homogeneous transform matrix using \autoref{eq:homomat}, and calculate the inverse of matrix:
\begin{equation}
\label{eq:reverseTrans2}
	T_R^G=
\begin{bmatrix}
R_L^G & P_L^G \\
\bf{0}   & 1 
\end{bmatrix} \cdot
\begin{bmatrix}
  (R_L^R)^\top & -(R_L^R)^{\top} P_L^R \\
\bf{0}   & 1 
\end{bmatrix}
\end{equation}


\begin{equation}
\label{eq:reverseTrans3}
	T_R^G=
\begin{bmatrix}
  R_L^G(R_L^R)^\top & -R_L^G(R_L^R)^{\top} P_L^R +  P_L^G\\
\bf{0}   & 1 
\end{bmatrix}
\end{equation}

By using the rotation matrix representation stated in \autoref{eq:rot}, the rotation matrix and translation matrix in $T_R^G$ is calculated in  \autoref{eq:reverseTrans4} and \autoref{eq:reverseTrans5}.
\begin{equation}
\label{eq:reverseTrans4}
	R_R^G=
	R_L^G(R_L^R)^\top =
  \Omega(\theta_{G} - \theta_{L}) 
\end{equation}

\begin{equation}
\label{eq:reverseTrans5}
	P_R^G=
	-R_R^G P_L^R +  P_L^G=
   -\Omega(\theta_{G} - \theta_{L}) \cdot
\begin{bmatrix}
  x_R\\
  y_R
\end{bmatrix}
+
\begin{bmatrix}
  x_G\\
  y_G
\end{bmatrix}
\end{equation}

Therefore, given the correspondence of $l_R$ and $l_G$,  the robot x-y position is $P_R^G$ and orientation is $\theta_{G} - \theta_{L}$. The number of possible robot pose generated depends on the occurrence of the landmarks in the field. For example, a miss match of a ``T'' junction triggered resampling will generate 14 possible robot poses by ``T'' junction. The weight for the newly generated hypothesis model is 1 divided by the number of occurrence of the landmarks in the field. So in the case of resampling by ``T'', the weight for the new hypothesis model is $1/14 = 0.0714$.

\section{Robot Pose}
In multi-hypothesis Kalman filter, there are multiple hypothesis models to form the possibility distribution. From the robot position probability distribution, one position $(x_{r}(t), y_{r}(t),\theta_{r}(t))$
has to be determined as the end result of localization algorithm. In the algorithm designed, the position is chosen to be the mean of the ``best'' hypothesis model in the probability distribution. The  ``best'' hypothesis model should satisfy the following two criteria:
\begin{itemize}
  \item Firstly, the hypothesis model should be globally best, which can be indicated by the weight of the model, the one with the highest weight is believed to be globally the best.
   \item Secondly, when multiple models have the same weights, then the quality of local belief is compared which can be indicated by the covariance of the Gaussian model. The one with lowest covariance is believed to be the best.
\end{itemize}

Covariance describes the confidence of the belief of the current Gaussian model. To compare the covariance value between the models, it is restructured as $Cov$ in \autoref{eq:coveq}. $Cov$ is the sum of the diagonal elements of the covariance matrix. Since the error in orientation can cause more negative influence on robot position, \ie the larger the error in orientation, the further the projected observed landmark will be away from its global correspondence, thus harder to match. Therefore, the variance value for orientation is doubled to have a higher weight in comparison.
\begin{equation}
\label{eq:coveq}
Cov = Cov_x + Cov_y +2*Cov_{\theta}
\end{equation}

\section{Model Pruning}
While the resampling step generates hypothesis models into the Gaussian sum distribution, pruning step is also necessary to remove the hypothesis models which are redundant or have little contribution to the whole distribution.

\subsection{Pruning by Weight}
\label{sub:Pruning by Weight}

\subsection{Pruning by Mahalanobis Distance}
\label{sub:Pruning by Mahalanobis Distance}



