\chapter{Analysis and Benchmarks\label{cha:chapter5}}
This chapter provides the results of the GPGPU implementation of the CCSDS 122.0-B-1 \gls{BPE} and also compares it with the state of the art reference and FPGA implementations.

\section{Methodology}

\subsection{Experimental setup}
The reference implementation of the encoder shall be run on the state of the art \unit[3.47]{GHz} single core Intel{\textregistered} Xeon{\texttrademark} for obtaining a host profile. The state of the art FPGA implementation from \cite{Manthey2014} is used for comparative analysis. The GPU used shall be NVIDIA{\textregistered} GeForce{\textregistered} GTX 670 launched in 2013 as a GPGPU platform to demonstrate the prototype. In a nutshell, the GPU card contains 7 NVIDIA Kepler GK104 \gls{SM}, each with 192 CUDA cores to sum up to a total of 1344 cores. Also, the presence of \unit[2048]{Mbyte} of global GPU memory ensures a significant storage to fit the large input image data of the orders of \unit[16]{k} by \unit[32]{k} pixel words.
\subsection{Scope}
The conformance single spectrum image set specified by the CCSDS 122.0-B-1 standard \cite{CCSDS122green} is used to test the correctness of the encoder. Four images having different pixel bit depths namely $coastal\_b4$, $ice\_2kb4$, $foc$ and $sar$ are chosen in this paper in order to have an extensive coverage of the encoder behavior. The encoded bit-streams are validated by decoding using a reference implementation of the CCSDS 122.0-B-1 decoder on the host machine. The input raw image data to the encoder and the output bit-stream from the decoder is checked for bit-wise match to ensure lossless reconstruction of the input image from the compressed bit-stream.
\subsection{Profiling tools}
The NVDIA command line profiler \textit{nvprof} is used to measure the execution times of the \gls{CUDA} \gls{GPGPU} implementation. The total execution time measured includes the \gls{CUDA} kernel execution time as well as the host/GPU to/from memory transfers. Regarding the host profile for the reference \gls{CPU} implementation, Linux clock/time API is used. The \gls{FPGA} profiling results are obtained using the cycle-accurate ModelSim{\texttrademark} simulator.
\section{Impact of image sizes on performance}
As already described in section~\ref{subsec:params}, the parallelism is directly dependent on the value of the NUM\_SEGMENTS. For normal sized images in the order 1024 by 1024 pixels, the NUM\_SEGMENTS is not significantly high to utilize the \gls{GPU} cores to good effect. Hence in order to improve the \gls{GPU} utilization, the input image has to be big enough to ensure high occupancy of the cores. Therefore, for benchmark purposes, bigger images are generated by concatenating the entire original image as a linear buffer $n$ times. Thus, the resultant image obtained is $n$ times the original image size, thereby increasing the occupancy of the GPU cores by virtue of increased value of NUM\_SEGMENTS. Four sets of images are created for $n=1,4,16,64$ for the performance analysis. Here, $n=1$ represents the original image as is. Fig~\ref{fig:ana:compper} shows that greater the image size, better is the achieved throughput.
%\begin{table}[h]
%\centering
%\begin{tabular}{|l|c|c|c|c|}
%\hline
%\multicolumn{1}{|c|}{\multirow{2}{*}{Image}} & \multicolumn{4}{c|}{Execution time (ms)} \\ \cline{2-5} 
%\multicolumn{1}{|c|}{}                       & $n=1$      & $n=4$    & $n=16$   & $n=64$  \\ \hline
%$coastal\_b4$                                & 470.105    & 240.129  & 257.062  & 595.475 \\ \hline
%$ice\_2kb4$                                  & 642.905    & 682.7    & 1321.17  & 3345.65 \\ \hline
%$foc$                                        & 404.915    & 161.477  & 147.659  & 309.959 \\ \hline
%$sar16bit$                                   & 899.52     & 503.762  & 244.176  & 413.907 \\ \hline
%\end{tabular}
%\caption{Comparison of Execution times based on Image sizes}
%\label{tab:exectime}
%\end{table}
\begin{figure}[tb]
  \centering
  \begin{subfigure}[b]{0.88\textwidth}
    \includegraphics[width=\textwidth]{performance}
    \caption{Comparison of Throughput based on Image sizes}
    \label{fig:ana:compper}
  \end{subfigure}
  \quad
  \begin{subfigure}[b]{0.88\textwidth}
    \includegraphics[width=\textwidth]{bpp}
    \caption{Comparison of Compression Efficiency based on Image sizes}
    \label{fig:ana:compbpp}
  \end{subfigure}
  \caption{Impact of image sizes on Compressor behavior}
  \label{fig:ana:compbeh}
\end{figure}

%\begin{figure}[tb]
%  \centering
%  \includegraphics[width=.7\textwidth]{pics/performance.pdf}
%  \caption{Comparison of Performance based on Image sizes}
%  \label{fig:impper:performance}
%\end{figure}

\section{Impact of image sizes on compression efficiency}
The impact of increased image size over compression efficiency is studied for the same four sets of images with $n=1,4,16,64$ as in the previous section. Fig.~\ref{fig:ana:compbpp} shows that the compression efficiency does not significantly suffer for large-sized concatenated images in comparison with the original image set.
%\begin{figure}[tb]
%  \centering
%  \includegraphics[width=.7\textwidth]{pics/bpp.pdf}
%  \caption{Comparison of Compression Efficiency based on Image sizes}
%  \label{fig:impce:compeff}
%\end{figure}

\section{Performance Benchmarks}
The throughput analysis is performed on the large sized images with $n=64$ case on CPU, GPGPU and FPGA platforms. It could be observed in Fig.~\ref{fig:benchmarks:plot}, that the GPGPU implementation achieves better throughput than the reference CPU implementation. However, the FPGA hardware implementation \cite{Manthey2014} achieves the best throughput in comparison with the CPU and GPGPU implementations. 
\begin{figure}[t]
  \centering
  \includegraphics[width=.98\textwidth]{benchmark_plot}
  \caption{Throughput comparison between CPU, GPGPU and FPGA implementations}
  \label{fig:benchmarks:plot}
\end{figure}
